{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"291.862266pt\" version=\"1.1\" viewBox=\"0 0 415.160156 291.862266\" width=\"415.160156pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 291.862266 \nL 415.160156 291.862266 \nL 415.160156 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 64.81875 230.008359 \nL 399.61875 230.008359 \nL 399.61875 12.568359 \nL 64.81875 12.568359 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 230.008359 \nL 64.81875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 4.15625 35.296875 \nQ 4.15625 48 6.765625 55.734375 \nQ 9.375 63.484375 14.515625 67.671875 \nQ 19.671875 71.875 27.484375 71.875 \nQ 33.25 71.875 37.59375 69.546875 \nQ 41.9375 67.234375 44.765625 62.859375 \nQ 47.609375 58.5 49.21875 52.21875 \nQ 50.828125 45.953125 50.828125 35.296875 \nQ 50.828125 22.703125 48.234375 14.96875 \nQ 45.65625 7.234375 40.5 3 \nQ 35.359375 -1.21875 27.484375 -1.21875 \nQ 17.140625 -1.21875 11.234375 6.203125 \nQ 4.15625 15.140625 4.15625 35.296875 \nz\nM 13.1875 35.296875 \nQ 13.1875 17.671875 17.3125 11.828125 \nQ 21.4375 6 27.484375 6 \nQ 33.546875 6 37.671875 11.859375 \nQ 41.796875 17.71875 41.796875 35.296875 \nQ 41.796875 52.984375 37.671875 58.78125 \nQ 33.546875 64.59375 27.390625 64.59375 \nQ 21.34375 64.59375 17.71875 59.46875 \nQ 13.1875 52.9375 13.1875 35.296875 \nz\n\" id=\"ArialMT-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(60.648047 250.245078)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 131.77875 230.008359 \nL 131.77875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 37.25 0 \nL 28.46875 0 \nL 28.46875 56 \nQ 25.296875 52.984375 20.140625 49.953125 \nQ 14.984375 46.921875 10.890625 45.40625 \nL 10.890625 53.90625 \nQ 18.265625 57.375 23.78125 62.296875 \nQ 29.296875 67.234375 31.59375 71.875 \nL 37.25 71.875 \nz\n\" id=\"ArialMT-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(123.437344 250.245078)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-49\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 198.73875 230.008359 \nL 198.73875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 50.34375 8.453125 \nL 50.34375 0 \nL 3.03125 0 \nQ 2.9375 3.171875 4.046875 6.109375 \nQ 5.859375 10.9375 9.828125 15.625 \nQ 13.8125 20.3125 21.34375 26.46875 \nQ 33.015625 36.03125 37.109375 41.625 \nQ 41.21875 47.21875 41.21875 52.203125 \nQ 41.21875 57.421875 37.46875 61 \nQ 33.734375 64.59375 27.734375 64.59375 \nQ 21.390625 64.59375 17.578125 60.78125 \nQ 13.765625 56.984375 13.71875 50.25 \nL 4.6875 51.171875 \nQ 5.609375 61.28125 11.65625 66.578125 \nQ 17.71875 71.875 27.9375 71.875 \nQ 38.234375 71.875 44.234375 66.15625 \nQ 50.25 60.453125 50.25 52 \nQ 50.25 47.703125 48.484375 43.546875 \nQ 46.734375 39.40625 42.65625 34.8125 \nQ 38.578125 30.21875 29.109375 22.21875 \nQ 21.1875 15.578125 18.9375 13.203125 \nQ 16.703125 10.84375 15.234375 8.453125 \nz\n\" id=\"ArialMT-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(190.397344 250.245078)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-50\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 265.69875 230.008359 \nL 265.69875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 4.203125 18.890625 \nL 12.984375 20.0625 \nQ 14.5 12.59375 18.140625 9.296875 \nQ 21.78125 6 27 6 \nQ 33.203125 6 37.46875 10.296875 \nQ 41.75 14.59375 41.75 20.953125 \nQ 41.75 27 37.796875 30.921875 \nQ 33.84375 34.859375 27.734375 34.859375 \nQ 25.25 34.859375 21.53125 33.890625 \nL 22.515625 41.609375 \nQ 23.390625 41.5 23.921875 41.5 \nQ 29.546875 41.5 34.03125 44.421875 \nQ 38.53125 47.359375 38.53125 53.46875 \nQ 38.53125 58.296875 35.25 61.46875 \nQ 31.984375 64.65625 26.8125 64.65625 \nQ 21.6875 64.65625 18.265625 61.421875 \nQ 14.84375 58.203125 13.875 51.765625 \nL 5.078125 53.328125 \nQ 6.6875 62.15625 12.390625 67.015625 \nQ 18.109375 71.875 26.609375 71.875 \nQ 32.46875 71.875 37.390625 69.359375 \nQ 42.328125 66.84375 44.9375 62.5 \nQ 47.5625 58.15625 47.5625 53.265625 \nQ 47.5625 48.640625 45.0625 44.828125 \nQ 42.578125 41.015625 37.703125 38.765625 \nQ 44.046875 37.3125 47.5625 32.6875 \nQ 51.078125 28.078125 51.078125 21.140625 \nQ 51.078125 11.765625 44.234375 5.25 \nQ 37.40625 -1.265625 26.953125 -1.265625 \nQ 17.53125 -1.265625 11.296875 4.34375 \nQ 5.078125 9.96875 4.203125 18.890625 \nz\n\" id=\"ArialMT-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(257.357344 250.245078)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-51\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 332.65875 230.008359 \nL 332.65875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 32.328125 0 \nL 32.328125 17.140625 \nL 1.265625 17.140625 \nL 1.265625 25.203125 \nL 33.9375 71.578125 \nL 41.109375 71.578125 \nL 41.109375 25.203125 \nL 50.78125 25.203125 \nL 50.78125 17.140625 \nL 41.109375 17.140625 \nL 41.109375 0 \nz\nM 32.328125 25.203125 \nL 32.328125 57.46875 \nL 9.90625 25.203125 \nz\n\" id=\"ArialMT-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(324.317344 250.245078)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-52\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 399.61875 230.008359 \nL 399.61875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 4.15625 18.75 \nL 13.375 19.53125 \nQ 14.40625 12.796875 18.140625 9.390625 \nQ 21.875 6 27.15625 6 \nQ 33.5 6 37.890625 10.78125 \nQ 42.28125 15.578125 42.28125 23.484375 \nQ 42.28125 31 38.0625 35.34375 \nQ 33.84375 39.703125 27 39.703125 \nQ 22.75 39.703125 19.328125 37.765625 \nQ 15.921875 35.84375 13.96875 32.765625 \nL 5.71875 33.84375 \nL 12.640625 70.609375 \nL 48.25 70.609375 \nL 48.25 62.203125 \nL 19.671875 62.203125 \nL 15.828125 42.96875 \nQ 22.265625 47.46875 29.34375 47.46875 \nQ 38.71875 47.46875 45.15625 40.96875 \nQ 51.609375 34.46875 51.609375 24.265625 \nQ 51.609375 14.546875 45.953125 7.46875 \nQ 39.0625 -1.21875 27.15625 -1.21875 \nQ 17.390625 -1.21875 11.203125 4.25 \nQ 5.03125 9.71875 4.15625 18.75 \nz\n\" id=\"ArialMT-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(391.277344 250.245078)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-53\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Reservations -->\n     <defs>\n      <path d=\"M 7.859375 0 \nL 7.859375 71.578125 \nL 39.59375 71.578125 \nQ 49.171875 71.578125 54.140625 69.640625 \nQ 59.125 67.71875 62.109375 62.828125 \nQ 65.09375 57.953125 65.09375 52.046875 \nQ 65.09375 44.4375 60.15625 39.203125 \nQ 55.21875 33.984375 44.921875 32.5625 \nQ 48.6875 30.765625 50.640625 29 \nQ 54.78125 25.203125 58.5 19.484375 \nL 70.953125 0 \nL 59.03125 0 \nL 49.5625 14.890625 \nQ 45.40625 21.34375 42.71875 24.75 \nQ 40.046875 28.171875 37.921875 29.53125 \nQ 35.796875 30.90625 33.59375 31.453125 \nQ 31.984375 31.78125 28.328125 31.78125 \nL 17.328125 31.78125 \nL 17.328125 0 \nz\nM 17.328125 39.984375 \nL 37.703125 39.984375 \nQ 44.1875 39.984375 47.84375 41.328125 \nQ 51.515625 42.671875 53.421875 45.625 \nQ 55.328125 48.578125 55.328125 52.046875 \nQ 55.328125 57.125 51.640625 60.390625 \nQ 47.953125 63.671875 39.984375 63.671875 \nL 17.328125 63.671875 \nz\n\" id=\"ArialMT-82\"/>\n      <path d=\"M 42.09375 16.703125 \nL 51.171875 15.578125 \nQ 49.03125 7.625 43.21875 3.21875 \nQ 37.40625 -1.171875 28.375 -1.171875 \nQ 17 -1.171875 10.328125 5.828125 \nQ 3.65625 12.84375 3.65625 25.484375 \nQ 3.65625 38.578125 10.390625 45.796875 \nQ 17.140625 53.03125 27.875 53.03125 \nQ 38.28125 53.03125 44.875 45.953125 \nQ 51.46875 38.875 51.46875 26.03125 \nQ 51.46875 25.25 51.421875 23.6875 \nL 12.75 23.6875 \nQ 13.234375 15.140625 17.578125 10.59375 \nQ 21.921875 6.0625 28.421875 6.0625 \nQ 33.25 6.0625 36.671875 8.59375 \nQ 40.09375 11.140625 42.09375 16.703125 \nz\nM 13.234375 30.90625 \nL 42.1875 30.90625 \nQ 41.609375 37.453125 38.875 40.71875 \nQ 34.671875 45.796875 27.984375 45.796875 \nQ 21.921875 45.796875 17.796875 41.75 \nQ 13.671875 37.703125 13.234375 30.90625 \nz\n\" id=\"ArialMT-101\"/>\n      <path d=\"M 3.078125 15.484375 \nL 11.765625 16.84375 \nQ 12.5 11.625 15.84375 8.84375 \nQ 19.1875 6.0625 25.203125 6.0625 \nQ 31.25 6.0625 34.171875 8.515625 \nQ 37.109375 10.984375 37.109375 14.3125 \nQ 37.109375 17.28125 34.515625 19 \nQ 32.71875 20.171875 25.53125 21.96875 \nQ 15.875 24.421875 12.140625 26.203125 \nQ 8.40625 27.984375 6.46875 31.125 \nQ 4.546875 34.28125 4.546875 38.09375 \nQ 4.546875 41.546875 6.125 44.5 \nQ 7.71875 47.46875 10.453125 49.421875 \nQ 12.5 50.921875 16.03125 51.96875 \nQ 19.578125 53.03125 23.640625 53.03125 \nQ 29.734375 53.03125 34.34375 51.265625 \nQ 38.96875 49.515625 41.15625 46.5 \nQ 43.359375 43.5 44.1875 38.484375 \nL 35.59375 37.3125 \nQ 35.015625 41.3125 32.203125 43.546875 \nQ 29.390625 45.796875 24.265625 45.796875 \nQ 18.21875 45.796875 15.625 43.796875 \nQ 13.03125 41.796875 13.03125 39.109375 \nQ 13.03125 37.40625 14.109375 36.03125 \nQ 15.1875 34.625 17.484375 33.6875 \nQ 18.796875 33.203125 25.25 31.453125 \nQ 34.578125 28.953125 38.25 27.359375 \nQ 41.9375 25.78125 44.03125 22.75 \nQ 46.140625 19.734375 46.140625 15.234375 \nQ 46.140625 10.84375 43.578125 6.953125 \nQ 41.015625 3.078125 36.171875 0.953125 \nQ 31.34375 -1.171875 25.25 -1.171875 \nQ 15.140625 -1.171875 9.84375 3.03125 \nQ 4.546875 7.234375 3.078125 15.484375 \nz\n\" id=\"ArialMT-115\"/>\n      <path d=\"M 6.5 0 \nL 6.5 51.859375 \nL 14.40625 51.859375 \nL 14.40625 44 \nQ 17.4375 49.515625 20 51.265625 \nQ 22.5625 53.03125 25.640625 53.03125 \nQ 30.078125 53.03125 34.671875 50.203125 \nL 31.640625 42.046875 \nQ 28.421875 43.953125 25.203125 43.953125 \nQ 22.3125 43.953125 20.015625 42.21875 \nQ 17.71875 40.484375 16.75 37.40625 \nQ 15.28125 32.71875 15.28125 27.15625 \nL 15.28125 0 \nz\n\" id=\"ArialMT-114\"/>\n      <path d=\"M 21 0 \nL 1.265625 51.859375 \nL 10.546875 51.859375 \nL 21.6875 20.796875 \nQ 23.484375 15.765625 25 10.359375 \nQ 26.171875 14.453125 28.265625 20.21875 \nL 39.796875 51.859375 \nL 48.828125 51.859375 \nL 29.203125 0 \nz\n\" id=\"ArialMT-118\"/>\n      <path d=\"M 40.4375 6.390625 \nQ 35.546875 2.25 31.03125 0.53125 \nQ 26.515625 -1.171875 21.34375 -1.171875 \nQ 12.796875 -1.171875 8.203125 3 \nQ 3.609375 7.171875 3.609375 13.671875 \nQ 3.609375 17.484375 5.34375 20.625 \nQ 7.078125 23.78125 9.890625 25.6875 \nQ 12.703125 27.59375 16.21875 28.5625 \nQ 18.796875 29.25 24.03125 29.890625 \nQ 34.671875 31.15625 39.703125 32.90625 \nQ 39.75 34.71875 39.75 35.203125 \nQ 39.75 40.578125 37.25 42.78125 \nQ 33.890625 45.75 27.25 45.75 \nQ 21.046875 45.75 18.09375 43.578125 \nQ 15.140625 41.40625 13.71875 35.890625 \nL 5.125 37.0625 \nQ 6.296875 42.578125 8.984375 45.96875 \nQ 11.671875 49.359375 16.75 51.1875 \nQ 21.828125 53.03125 28.515625 53.03125 \nQ 35.15625 53.03125 39.296875 51.46875 \nQ 43.453125 49.90625 45.40625 47.53125 \nQ 47.359375 45.171875 48.140625 41.546875 \nQ 48.578125 39.3125 48.578125 33.453125 \nL 48.578125 21.734375 \nQ 48.578125 9.46875 49.140625 6.21875 \nQ 49.703125 2.984375 51.375 0 \nL 42.1875 0 \nQ 40.828125 2.734375 40.4375 6.390625 \nz\nM 39.703125 26.03125 \nQ 34.90625 24.078125 25.34375 22.703125 \nQ 19.921875 21.921875 17.671875 20.9375 \nQ 15.4375 19.96875 14.203125 18.09375 \nQ 12.984375 16.21875 12.984375 13.921875 \nQ 12.984375 10.40625 15.640625 8.0625 \nQ 18.3125 5.71875 23.4375 5.71875 \nQ 28.515625 5.71875 32.46875 7.9375 \nQ 36.421875 10.15625 38.28125 14.015625 \nQ 39.703125 17 39.703125 22.796875 \nz\n\" id=\"ArialMT-97\"/>\n      <path d=\"M 25.78125 7.859375 \nL 27.046875 0.09375 \nQ 23.34375 -0.6875 20.40625 -0.6875 \nQ 15.625 -0.6875 12.984375 0.828125 \nQ 10.359375 2.34375 9.28125 4.8125 \nQ 8.203125 7.28125 8.203125 15.1875 \nL 8.203125 45.015625 \nL 1.765625 45.015625 \nL 1.765625 51.859375 \nL 8.203125 51.859375 \nL 8.203125 64.703125 \nL 16.9375 69.96875 \nL 16.9375 51.859375 \nL 25.78125 51.859375 \nL 25.78125 45.015625 \nL 16.9375 45.015625 \nL 16.9375 14.703125 \nQ 16.9375 10.9375 17.40625 9.859375 \nQ 17.875 8.796875 18.921875 8.15625 \nQ 19.96875 7.515625 21.921875 7.515625 \nQ 23.390625 7.515625 25.78125 7.859375 \nz\n\" id=\"ArialMT-116\"/>\n      <path d=\"M 6.640625 61.46875 \nL 6.640625 71.578125 \nL 15.4375 71.578125 \nL 15.4375 61.46875 \nz\nM 6.640625 0 \nL 6.640625 51.859375 \nL 15.4375 51.859375 \nL 15.4375 0 \nz\n\" id=\"ArialMT-105\"/>\n      <path d=\"M 3.328125 25.921875 \nQ 3.328125 40.328125 11.328125 47.265625 \nQ 18.015625 53.03125 27.640625 53.03125 \nQ 38.328125 53.03125 45.109375 46.015625 \nQ 51.90625 39.015625 51.90625 26.65625 \nQ 51.90625 16.65625 48.90625 10.90625 \nQ 45.90625 5.171875 40.15625 2 \nQ 34.421875 -1.171875 27.640625 -1.171875 \nQ 16.75 -1.171875 10.03125 5.8125 \nQ 3.328125 12.796875 3.328125 25.921875 \nz\nM 12.359375 25.921875 \nQ 12.359375 15.96875 16.703125 11.015625 \nQ 21.046875 6.0625 27.640625 6.0625 \nQ 34.1875 6.0625 38.53125 11.03125 \nQ 42.875 16.015625 42.875 26.21875 \nQ 42.875 35.84375 38.5 40.796875 \nQ 34.125 45.75 27.640625 45.75 \nQ 21.046875 45.75 16.703125 40.8125 \nQ 12.359375 35.890625 12.359375 25.921875 \nz\n\" id=\"ArialMT-111\"/>\n      <path d=\"M 6.59375 0 \nL 6.59375 51.859375 \nL 14.5 51.859375 \nL 14.5 44.484375 \nQ 20.21875 53.03125 31 53.03125 \nQ 35.6875 53.03125 39.625 51.34375 \nQ 43.5625 49.65625 45.515625 46.921875 \nQ 47.46875 44.1875 48.25 40.4375 \nQ 48.734375 37.984375 48.734375 31.890625 \nL 48.734375 0 \nL 39.9375 0 \nL 39.9375 31.546875 \nQ 39.9375 36.921875 38.90625 39.578125 \nQ 37.890625 42.234375 35.28125 43.8125 \nQ 32.671875 45.40625 29.15625 45.40625 \nQ 23.53125 45.40625 19.453125 41.84375 \nQ 15.375 38.28125 15.375 28.328125 \nL 15.375 0 \nz\n\" id=\"ArialMT-110\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(144.684375 278.699766)scale(0.3 -0.3)\">\n      <use xlink:href=\"#ArialMT-82\"/>\n      <use x=\"72.216797\" xlink:href=\"#ArialMT-101\"/>\n      <use x=\"127.832031\" xlink:href=\"#ArialMT-115\"/>\n      <use x=\"177.832031\" xlink:href=\"#ArialMT-101\"/>\n      <use x=\"233.447266\" xlink:href=\"#ArialMT-114\"/>\n      <use x=\"266.748047\" xlink:href=\"#ArialMT-118\"/>\n      <use x=\"316.748047\" xlink:href=\"#ArialMT-97\"/>\n      <use x=\"372.363281\" xlink:href=\"#ArialMT-116\"/>\n      <use x=\"400.146484\" xlink:href=\"#ArialMT-105\"/>\n      <use x=\"422.363281\" xlink:href=\"#ArialMT-111\"/>\n      <use x=\"477.978516\" xlink:href=\"#ArialMT-110\"/>\n      <use x=\"533.59375\" xlink:href=\"#ArialMT-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 230.008359 \nL 399.61875 230.008359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(46.977344 235.376719)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 186.520359 \nL 399.61875 186.520359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g style=\"fill:#262626;\" transform=\"translate(38.635938 191.888719)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-49\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 143.032359 \nL 399.61875 143.032359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(38.635938 148.400719)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-50\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 99.544359 \nL 399.61875 99.544359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 30 -->\n      <g style=\"fill:#262626;\" transform=\"translate(38.635938 104.912719)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-51\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 56.056359 \nL 399.61875 56.056359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(38.635938 61.424719)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-52\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#pa59ce1008f)\" d=\"M 64.81875 12.568359 \nL 399.61875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(38.635938 17.936719)scale(0.15 -0.15)\">\n       <use xlink:href=\"#ArialMT-53\"/>\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Pizzas -->\n     <defs>\n      <path d=\"M 7.71875 0 \nL 7.71875 71.578125 \nL 34.71875 71.578125 \nQ 41.84375 71.578125 45.609375 70.90625 \nQ 50.875 70.015625 54.4375 67.546875 \nQ 58.015625 65.09375 60.1875 60.640625 \nQ 62.359375 56.203125 62.359375 50.875 \nQ 62.359375 41.75 56.546875 35.421875 \nQ 50.734375 29.109375 35.546875 29.109375 \nL 17.1875 29.109375 \nL 17.1875 0 \nz\nM 17.1875 37.546875 \nL 35.6875 37.546875 \nQ 44.875 37.546875 48.734375 40.96875 \nQ 52.59375 44.390625 52.59375 50.59375 \nQ 52.59375 55.078125 50.3125 58.265625 \nQ 48.046875 61.46875 44.34375 62.5 \nQ 41.9375 63.140625 35.5 63.140625 \nL 17.1875 63.140625 \nz\n\" id=\"ArialMT-80\"/>\n      <path d=\"M 1.953125 0 \nL 1.953125 7.125 \nL 34.96875 45.015625 \nQ 29.34375 44.734375 25.046875 44.734375 \nL 3.90625 44.734375 \nL 3.90625 51.859375 \nL 46.296875 51.859375 \nL 46.296875 46.046875 \nL 18.21875 13.140625 \nL 12.796875 7.125 \nQ 18.703125 7.5625 23.875 7.5625 \nL 47.859375 7.5625 \nL 47.859375 0 \nz\n\" id=\"ArialMT-122\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(28.673438 165.468047)rotate(-90)scale(0.3 -0.3)\">\n      <use xlink:href=\"#ArialMT-80\"/>\n      <use x=\"66.699219\" xlink:href=\"#ArialMT-105\"/>\n      <use x=\"88.916016\" xlink:href=\"#ArialMT-122\"/>\n      <use x=\"138.916016\" xlink:href=\"#ArialMT-122\"/>\n      <use x=\"188.916016\" xlink:href=\"#ArialMT-97\"/>\n      <use x=\"244.53125\" xlink:href=\"#ArialMT-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m8583ba5823\" style=\"stroke:#4c72b0;\"/>\n    </defs>\n    <g clip-path=\"url(#pa59ce1008f)\">\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"151.86675\" xlink:href=\"#m8583ba5823\" y=\"86.497959\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"78.21075\" xlink:href=\"#m8583ba5823\" y=\"160.427559\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"158.56275\" xlink:href=\"#m8583ba5823\" y=\"90.846759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"218.82675\" xlink:href=\"#m8583ba5823\" y=\"8.219559\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"151.86675\" xlink:href=\"#m8583ba5823\" y=\"112.590759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"71.51475\" xlink:href=\"#m8583ba5823\" y=\"160.427559\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"185.34675\" xlink:href=\"#m8583ba5823\" y=\"82.149159\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"131.77875\" xlink:href=\"#m8583ba5823\" y=\"156.078759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"238.91475\" xlink:href=\"#m8583ba5823\" y=\"103.893159\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"84.90675\" xlink:href=\"#m8583ba5823\" y=\"164.776359\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"84.90675\" xlink:href=\"#m8583ba5823\" y=\"164.776359\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"205.43475\" xlink:href=\"#m8583ba5823\" y=\"90.846759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"111.69075\" xlink:href=\"#m8583ba5823\" y=\"134.334759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"212.13075\" xlink:href=\"#m8583ba5823\" y=\"69.102759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"78.21075\" xlink:href=\"#m8583ba5823\" y=\"173.473959\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"245.61075\" xlink:href=\"#m8583ba5823\" y=\"38.661159\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"104.99475\" xlink:href=\"#m8583ba5823\" y=\"160.427559\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"131.77875\" xlink:href=\"#m8583ba5823\" y=\"138.683559\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"185.34675\" xlink:href=\"#m8583ba5823\" y=\"69.102759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"165.25875\" xlink:href=\"#m8583ba5823\" y=\"99.544359\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"125.08275\" xlink:href=\"#m8583ba5823\" y=\"116.939559\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"238.91475\" xlink:href=\"#m8583ba5823\" y=\"82.149159\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"118.38675\" xlink:href=\"#m8583ba5823\" y=\"129.985959\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"165.25875\" xlink:href=\"#m8583ba5823\" y=\"60.405159\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"131.77875\" xlink:href=\"#m8583ba5823\" y=\"112.590759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"205.43475\" xlink:href=\"#m8583ba5823\" y=\"69.102759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"98.29875\" xlink:href=\"#m8583ba5823\" y=\"156.078759\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"104.99475\" xlink:href=\"#m8583ba5823\" y=\"151.729959\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"151.86675\" xlink:href=\"#m8583ba5823\" y=\"121.288359\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"151.86675\" xlink:href=\"#m8583ba5823\" y=\"129.985959\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 64.81875 230.008359 \nL 64.81875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 399.61875 230.008359 \nL 399.61875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 64.81875 230.008359 \nL 399.61875 230.008359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 64.81875 12.568359 \nL 399.61875 12.568359 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa59ce1008f\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"64.81875\" y=\"12.568359\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sns.set()\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Reservations\", fontsize=30)\n",
    "plt.ylabel(\"Pizzas\", fontsize=30)\n",
    "X, Y = np.loadtxt('/Users/thomasshorney/code/02_first/pizza.txt', skiprows=1, unpack=True)\n",
    "plt.plot(X, Y, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return X * w\n",
    "\n",
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X,Y, w)\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, current_loss))\n",
    "\n",
    "        if loss(X, Y, w + lr) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X, Y, w - lr) < current_loss:\n",
    "            w -= lr\n",
    "        else:\n",
    "            return w\n",
    "    raise Exception(\"Couldn't converge within %d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration    0 => Loss: 812.866667\nIteration    1 => Loss: 804.820547\nIteration    2 => Loss: 796.818187\nIteration    3 => Loss: 788.859587\nIteration    4 => Loss: 780.944747\nIteration    5 => Loss: 773.073667\nIteration    6 => Loss: 765.246347\nIteration    7 => Loss: 757.462787\nIteration    8 => Loss: 749.722987\nIteration    9 => Loss: 742.026947\nIteration   10 => Loss: 734.374667\nIteration   11 => Loss: 726.766147\nIteration   12 => Loss: 719.201387\nIteration   13 => Loss: 711.680387\nIteration   14 => Loss: 704.203147\nIteration   15 => Loss: 696.769667\nIteration   16 => Loss: 689.379947\nIteration   17 => Loss: 682.033987\nIteration   18 => Loss: 674.731787\nIteration   19 => Loss: 667.473347\nIteration   20 => Loss: 660.258667\nIteration   21 => Loss: 653.087747\nIteration   22 => Loss: 645.960587\nIteration   23 => Loss: 638.877187\nIteration   24 => Loss: 631.837547\nIteration   25 => Loss: 624.841667\nIteration   26 => Loss: 617.889547\nIteration   27 => Loss: 610.981187\nIteration   28 => Loss: 604.116587\nIteration   29 => Loss: 597.295747\nIteration   30 => Loss: 590.518667\nIteration   31 => Loss: 583.785347\nIteration   32 => Loss: 577.095787\nIteration   33 => Loss: 570.449987\nIteration   34 => Loss: 563.847947\nIteration   35 => Loss: 557.289667\nIteration   36 => Loss: 550.775147\nIteration   37 => Loss: 544.304387\nIteration   38 => Loss: 537.877387\nIteration   39 => Loss: 531.494147\nIteration   40 => Loss: 525.154667\nIteration   41 => Loss: 518.858947\nIteration   42 => Loss: 512.606987\nIteration   43 => Loss: 506.398787\nIteration   44 => Loss: 500.234347\nIteration   45 => Loss: 494.113667\nIteration   46 => Loss: 488.036747\nIteration   47 => Loss: 482.003587\nIteration   48 => Loss: 476.014187\nIteration   49 => Loss: 470.068547\nIteration   50 => Loss: 464.166667\nIteration   51 => Loss: 458.308547\nIteration   52 => Loss: 452.494187\nIteration   53 => Loss: 446.723587\nIteration   54 => Loss: 440.996747\nIteration   55 => Loss: 435.313667\nIteration   56 => Loss: 429.674347\nIteration   57 => Loss: 424.078787\nIteration   58 => Loss: 418.526987\nIteration   59 => Loss: 413.018947\nIteration   60 => Loss: 407.554667\nIteration   61 => Loss: 402.134147\nIteration   62 => Loss: 396.757387\nIteration   63 => Loss: 391.424387\nIteration   64 => Loss: 386.135147\nIteration   65 => Loss: 380.889667\nIteration   66 => Loss: 375.687947\nIteration   67 => Loss: 370.529987\nIteration   68 => Loss: 365.415787\nIteration   69 => Loss: 360.345347\nIteration   70 => Loss: 355.318667\nIteration   71 => Loss: 350.335747\nIteration   72 => Loss: 345.396587\nIteration   73 => Loss: 340.501187\nIteration   74 => Loss: 335.649547\nIteration   75 => Loss: 330.841667\nIteration   76 => Loss: 326.077547\nIteration   77 => Loss: 321.357187\nIteration   78 => Loss: 316.680587\nIteration   79 => Loss: 312.047747\nIteration   80 => Loss: 307.458667\nIteration   81 => Loss: 302.913347\nIteration   82 => Loss: 298.411787\nIteration   83 => Loss: 293.953987\nIteration   84 => Loss: 289.539947\nIteration   85 => Loss: 285.169667\nIteration   86 => Loss: 280.843147\nIteration   87 => Loss: 276.560387\nIteration   88 => Loss: 272.321387\nIteration   89 => Loss: 268.126147\nIteration   90 => Loss: 263.974667\nIteration   91 => Loss: 259.866947\nIteration   92 => Loss: 255.802987\nIteration   93 => Loss: 251.782787\nIteration   94 => Loss: 247.806347\nIteration   95 => Loss: 243.873667\nIteration   96 => Loss: 239.984747\nIteration   97 => Loss: 236.139587\nIteration   98 => Loss: 232.338187\nIteration   99 => Loss: 228.580547\nIteration  100 => Loss: 224.866667\nIteration  101 => Loss: 221.196547\nIteration  102 => Loss: 217.570187\nIteration  103 => Loss: 213.987587\nIteration  104 => Loss: 210.448747\nIteration  105 => Loss: 206.953667\nIteration  106 => Loss: 203.502347\nIteration  107 => Loss: 200.094787\nIteration  108 => Loss: 196.730987\nIteration  109 => Loss: 193.410947\nIteration  110 => Loss: 190.134667\nIteration  111 => Loss: 186.902147\nIteration  112 => Loss: 183.713387\nIteration  113 => Loss: 180.568387\nIteration  114 => Loss: 177.467147\nIteration  115 => Loss: 174.409667\nIteration  116 => Loss: 171.395947\nIteration  117 => Loss: 168.425987\nIteration  118 => Loss: 165.499787\nIteration  119 => Loss: 162.617347\nIteration  120 => Loss: 159.778667\nIteration  121 => Loss: 156.983747\nIteration  122 => Loss: 154.232587\nIteration  123 => Loss: 151.525187\nIteration  124 => Loss: 148.861547\nIteration  125 => Loss: 146.241667\nIteration  126 => Loss: 143.665547\nIteration  127 => Loss: 141.133187\nIteration  128 => Loss: 138.644587\nIteration  129 => Loss: 136.199747\nIteration  130 => Loss: 133.798667\nIteration  131 => Loss: 131.441347\nIteration  132 => Loss: 129.127787\nIteration  133 => Loss: 126.857987\nIteration  134 => Loss: 124.631947\nIteration  135 => Loss: 122.449667\nIteration  136 => Loss: 120.311147\nIteration  137 => Loss: 118.216387\nIteration  138 => Loss: 116.165387\nIteration  139 => Loss: 114.158147\nIteration  140 => Loss: 112.194667\nIteration  141 => Loss: 110.274947\nIteration  142 => Loss: 108.398987\nIteration  143 => Loss: 106.566787\nIteration  144 => Loss: 104.778347\nIteration  145 => Loss: 103.033667\nIteration  146 => Loss: 101.332747\nIteration  147 => Loss: 99.675587\nIteration  148 => Loss: 98.062187\nIteration  149 => Loss: 96.492547\nIteration  150 => Loss: 94.966667\nIteration  151 => Loss: 93.484547\nIteration  152 => Loss: 92.046187\nIteration  153 => Loss: 90.651587\nIteration  154 => Loss: 89.300747\nIteration  155 => Loss: 87.993667\nIteration  156 => Loss: 86.730347\nIteration  157 => Loss: 85.510787\nIteration  158 => Loss: 84.334987\nIteration  159 => Loss: 83.202947\nIteration  160 => Loss: 82.114667\nIteration  161 => Loss: 81.070147\nIteration  162 => Loss: 80.069387\nIteration  163 => Loss: 79.112387\nIteration  164 => Loss: 78.199147\nIteration  165 => Loss: 77.329667\nIteration  166 => Loss: 76.503947\nIteration  167 => Loss: 75.721987\nIteration  168 => Loss: 74.983787\nIteration  169 => Loss: 74.289347\nIteration  170 => Loss: 73.638667\nIteration  171 => Loss: 73.031747\nIteration  172 => Loss: 72.468587\nIteration  173 => Loss: 71.949187\nIteration  174 => Loss: 71.473547\nIteration  175 => Loss: 71.041667\nIteration  176 => Loss: 70.653547\nIteration  177 => Loss: 70.309187\nIteration  178 => Loss: 70.008587\nIteration  179 => Loss: 69.751747\nIteration  180 => Loss: 69.538667\nIteration  181 => Loss: 69.369347\nIteration  182 => Loss: 69.243787\nIteration  183 => Loss: 69.161987\nIteration  184 => Loss: 69.123947\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.8400000000000014"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train(X, Y,1000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing Bias\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return X * w + b\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(\"Iteration %4d => Loss:%.6f\" %(i, current_loss))\n",
    "\n",
    "        if loss(X, Y, w + lr, b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X, Y, w - lr, b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X, Y, w, b + lr) < current_loss:\n",
    "            b += lr\n",
    "        elif loss(X, Y, w, b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        else:\n",
    "            return w, b\n",
    "\n",
    "    Exception(\"Couldn't converge within %d iterations\" % iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\nIteration  394 => Loss:56.195020\nIteration  395 => Loss:56.135853\nIteration  396 => Loss:56.076887\nIteration  397 => Loss:56.018120\nIteration  398 => Loss:55.959553\nIteration  399 => Loss:55.901187\nIteration  400 => Loss:55.843020\nIteration  401 => Loss:55.785053\nIteration  402 => Loss:55.782587\nIteration  403 => Loss:55.722287\nIteration  404 => Loss:55.662187\nIteration  405 => Loss:55.602287\nIteration  406 => Loss:55.542587\nIteration  407 => Loss:55.483087\nIteration  408 => Loss:55.423787\nIteration  409 => Loss:55.364687\nIteration  410 => Loss:55.305787\nIteration  411 => Loss:55.247087\nIteration  412 => Loss:55.188587\nIteration  413 => Loss:55.130287\nIteration  414 => Loss:55.072187\nIteration  415 => Loss:55.014287\nIteration  416 => Loss:54.956587\nIteration  417 => Loss:54.899087\nIteration  418 => Loss:54.841787\nIteration  419 => Loss:54.784687\nIteration  420 => Loss:54.782913\nIteration  421 => Loss:54.723480\nIteration  422 => Loss:54.664247\nIteration  423 => Loss:54.605213\nIteration  424 => Loss:54.546380\nIteration  425 => Loss:54.487747\nIteration  426 => Loss:54.429313\nIteration  427 => Loss:54.371080\nIteration  428 => Loss:54.313047\nIteration  429 => Loss:54.255213\nIteration  430 => Loss:54.197580\nIteration  431 => Loss:54.140147\nIteration  432 => Loss:54.082913\nIteration  433 => Loss:54.025880\nIteration  434 => Loss:53.969047\nIteration  435 => Loss:53.912413\nIteration  436 => Loss:53.855980\nIteration  437 => Loss:53.799747\nIteration  438 => Loss:53.798667\nIteration  439 => Loss:53.740100\nIteration  440 => Loss:53.681733\nIteration  441 => Loss:53.623567\nIteration  442 => Loss:53.565600\nIteration  443 => Loss:53.507833\nIteration  444 => Loss:53.450267\nIteration  445 => Loss:53.392900\nIteration  446 => Loss:53.335733\nIteration  447 => Loss:53.278767\nIteration  448 => Loss:53.222000\nIteration  449 => Loss:53.165433\nIteration  450 => Loss:53.109067\nIteration  451 => Loss:53.052900\nIteration  452 => Loss:52.996933\nIteration  453 => Loss:52.941167\nIteration  454 => Loss:52.885600\nIteration  455 => Loss:52.830233\nIteration  456 => Loss:52.829847\nIteration  457 => Loss:52.772147\nIteration  458 => Loss:52.714647\nIteration  459 => Loss:52.657347\nIteration  460 => Loss:52.600247\nIteration  461 => Loss:52.543347\nIteration  462 => Loss:52.486647\nIteration  463 => Loss:52.430147\nIteration  464 => Loss:52.373847\nIteration  465 => Loss:52.317747\nIteration  466 => Loss:52.261847\nIteration  467 => Loss:52.206147\nIteration  468 => Loss:52.150647\nIteration  469 => Loss:52.095347\nIteration  470 => Loss:52.040247\nIteration  471 => Loss:51.985347\nIteration  472 => Loss:51.930647\nIteration  473 => Loss:51.876147\nIteration  474 => Loss:51.821847\nIteration  475 => Loss:51.819620\nIteration  476 => Loss:51.762987\nIteration  477 => Loss:51.706553\nIteration  478 => Loss:51.650320\nIteration  479 => Loss:51.594287\nIteration  480 => Loss:51.538453\nIteration  481 => Loss:51.482820\nIteration  482 => Loss:51.427387\nIteration  483 => Loss:51.372153\nIteration  484 => Loss:51.317120\nIteration  485 => Loss:51.262287\nIteration  486 => Loss:51.207653\nIteration  487 => Loss:51.153220\nIteration  488 => Loss:51.098987\nIteration  489 => Loss:51.044953\nIteration  490 => Loss:50.991120\nIteration  491 => Loss:50.937487\nIteration  492 => Loss:50.884053\nIteration  493 => Loss:50.882520\nIteration  494 => Loss:50.826753\nIteration  495 => Loss:50.771187\nIteration  496 => Loss:50.715820\nIteration  497 => Loss:50.660653\nIteration  498 => Loss:50.605687\nIteration  499 => Loss:50.550920\nIteration  500 => Loss:50.496353\nIteration  501 => Loss:50.441987\nIteration  502 => Loss:50.387820\nIteration  503 => Loss:50.333853\nIteration  504 => Loss:50.280087\nIteration  505 => Loss:50.226520\nIteration  506 => Loss:50.173153\nIteration  507 => Loss:50.119987\nIteration  508 => Loss:50.067020\nIteration  509 => Loss:50.014253\nIteration  510 => Loss:49.961687\nIteration  511 => Loss:49.960847\nIteration  512 => Loss:49.905947\nIteration  513 => Loss:49.851247\nIteration  514 => Loss:49.796747\nIteration  515 => Loss:49.742447\nIteration  516 => Loss:49.688347\nIteration  517 => Loss:49.634447\nIteration  518 => Loss:49.580747\nIteration  519 => Loss:49.527247\nIteration  520 => Loss:49.473947\nIteration  521 => Loss:49.420847\nIteration  522 => Loss:49.367947\nIteration  523 => Loss:49.315247\nIteration  524 => Loss:49.262747\nIteration  525 => Loss:49.210447\nIteration  526 => Loss:49.158347\nIteration  527 => Loss:49.106447\nIteration  528 => Loss:49.054747\nIteration  529 => Loss:49.054600\nIteration  530 => Loss:49.000567\nIteration  531 => Loss:48.946733\nIteration  532 => Loss:48.893100\nIteration  533 => Loss:48.839667\nIteration  534 => Loss:48.786433\nIteration  535 => Loss:48.733400\nIteration  536 => Loss:48.680567\nIteration  537 => Loss:48.627933\nIteration  538 => Loss:48.575500\nIteration  539 => Loss:48.523267\nIteration  540 => Loss:48.471233\nIteration  541 => Loss:48.419400\nIteration  542 => Loss:48.367767\nIteration  543 => Loss:48.316333\nIteration  544 => Loss:48.265100\nIteration  545 => Loss:48.214067\nIteration  546 => Loss:48.163233\nIteration  547 => Loss:48.112600\nIteration  548 => Loss:48.110613\nIteration  549 => Loss:48.057647\nIteration  550 => Loss:48.004880\nIteration  551 => Loss:47.952313\nIteration  552 => Loss:47.899947\nIteration  553 => Loss:47.847780\nIteration  554 => Loss:47.795813\nIteration  555 => Loss:47.744047\nIteration  556 => Loss:47.692480\nIteration  557 => Loss:47.641113\nIteration  558 => Loss:47.589947\nIteration  559 => Loss:47.538980\nIteration  560 => Loss:47.488213\nIteration  561 => Loss:47.437647\nIteration  562 => Loss:47.387280\nIteration  563 => Loss:47.337113\nIteration  564 => Loss:47.287147\nIteration  565 => Loss:47.237380\nIteration  566 => Loss:47.236087\nIteration  567 => Loss:47.183987\nIteration  568 => Loss:47.132087\nIteration  569 => Loss:47.080387\nIteration  570 => Loss:47.028887\nIteration  571 => Loss:46.977587\nIteration  572 => Loss:46.926487\nIteration  573 => Loss:46.875587\nIteration  574 => Loss:46.824887\nIteration  575 => Loss:46.774387\nIteration  576 => Loss:46.724087\nIteration  577 => Loss:46.673987\nIteration  578 => Loss:46.624087\nIteration  579 => Loss:46.574387\nIteration  580 => Loss:46.524887\nIteration  581 => Loss:46.475587\nIteration  582 => Loss:46.426487\nIteration  583 => Loss:46.377587\nIteration  584 => Loss:46.376987\nIteration  585 => Loss:46.325753\nIteration  586 => Loss:46.274720\nIteration  587 => Loss:46.223887\nIteration  588 => Loss:46.173253\nIteration  589 => Loss:46.122820\nIteration  590 => Loss:46.072587\nIteration  591 => Loss:46.022553\nIteration  592 => Loss:45.972720\nIteration  593 => Loss:45.923087\nIteration  594 => Loss:45.873653\nIteration  595 => Loss:45.824420\nIteration  596 => Loss:45.775387\nIteration  597 => Loss:45.726553\nIteration  598 => Loss:45.677920\nIteration  599 => Loss:45.629487\nIteration  600 => Loss:45.581253\nIteration  601 => Loss:45.533220\nIteration  602 => Loss:45.485387\nIteration  603 => Loss:45.482947\nIteration  604 => Loss:45.432780\nIteration  605 => Loss:45.382813\nIteration  606 => Loss:45.333047\nIteration  607 => Loss:45.283480\nIteration  608 => Loss:45.234113\nIteration  609 => Loss:45.184947\nIteration  610 => Loss:45.135980\nIteration  611 => Loss:45.087213\nIteration  612 => Loss:45.038647\nIteration  613 => Loss:44.990280\nIteration  614 => Loss:44.942113\nIteration  615 => Loss:44.894147\nIteration  616 => Loss:44.846380\nIteration  617 => Loss:44.798813\nIteration  618 => Loss:44.751447\nIteration  619 => Loss:44.704280\nIteration  620 => Loss:44.657313\nIteration  621 => Loss:44.655567\nIteration  622 => Loss:44.606267\nIteration  623 => Loss:44.557167\nIteration  624 => Loss:44.508267\nIteration  625 => Loss:44.459567\nIteration  626 => Loss:44.411067\nIteration  627 => Loss:44.362767\nIteration  628 => Loss:44.314667\nIteration  629 => Loss:44.266767\nIteration  630 => Loss:44.219067\nIteration  631 => Loss:44.171567\nIteration  632 => Loss:44.124267\nIteration  633 => Loss:44.077167\nIteration  634 => Loss:44.030267\nIteration  635 => Loss:43.983567\nIteration  636 => Loss:43.937067\nIteration  637 => Loss:43.890767\nIteration  638 => Loss:43.844667\nIteration  639 => Loss:43.843613\nIteration  640 => Loss:43.795180\nIteration  641 => Loss:43.746947\nIteration  642 => Loss:43.698913\nIteration  643 => Loss:43.651080\nIteration  644 => Loss:43.603447\nIteration  645 => Loss:43.556013\nIteration  646 => Loss:43.508780\nIteration  647 => Loss:43.461747\nIteration  648 => Loss:43.414913\nIteration  649 => Loss:43.368280\nIteration  650 => Loss:43.321847\nIteration  651 => Loss:43.275613\nIteration  652 => Loss:43.229580\nIteration  653 => Loss:43.183747\nIteration  654 => Loss:43.138113\nIteration  655 => Loss:43.092680\nIteration  656 => Loss:43.047447\nIteration  657 => Loss:43.047087\nIteration  658 => Loss:42.999520\nIteration  659 => Loss:42.952153\nIteration  660 => Loss:42.904987\nIteration  661 => Loss:42.858020\nIteration  662 => Loss:42.811253\nIteration  663 => Loss:42.764687\nIteration  664 => Loss:42.718320\nIteration  665 => Loss:42.672153\nIteration  666 => Loss:42.626187\nIteration  667 => Loss:42.580420\nIteration  668 => Loss:42.534853\nIteration  669 => Loss:42.489487\nIteration  670 => Loss:42.444320\nIteration  671 => Loss:42.399353\nIteration  672 => Loss:42.354587\nIteration  673 => Loss:42.310020\nIteration  674 => Loss:42.265653\nIteration  675 => Loss:42.221487\nIteration  676 => Loss:42.219287\nIteration  677 => Loss:42.172787\nIteration  678 => Loss:42.126487\nIteration  679 => Loss:42.080387\nIteration  680 => Loss:42.034487\nIteration  681 => Loss:41.988787\nIteration  682 => Loss:41.943287\nIteration  683 => Loss:41.897987\nIteration  684 => Loss:41.852887\nIteration  685 => Loss:41.807987\nIteration  686 => Loss:41.763287\nIteration  687 => Loss:41.718787\nIteration  688 => Loss:41.674487\nIteration  689 => Loss:41.630387\nIteration  690 => Loss:41.586487\nIteration  691 => Loss:41.542787\nIteration  692 => Loss:41.499287\nIteration  693 => Loss:41.455987\nIteration  694 => Loss:41.454480\nIteration  695 => Loss:41.408847\nIteration  696 => Loss:41.363413\nIteration  697 => Loss:41.318180\nIteration  698 => Loss:41.273147\nIteration  699 => Loss:41.228313\nIteration  700 => Loss:41.183680\nIteration  701 => Loss:41.139247\nIteration  702 => Loss:41.095013\nIteration  703 => Loss:41.050980\nIteration  704 => Loss:41.007147\nIteration  705 => Loss:40.963513\nIteration  706 => Loss:40.920080\nIteration  707 => Loss:40.876847\nIteration  708 => Loss:40.833813\nIteration  709 => Loss:40.790980\nIteration  710 => Loss:40.748347\nIteration  711 => Loss:40.705913\nIteration  712 => Loss:40.705100\nIteration  713 => Loss:40.660333\nIteration  714 => Loss:40.615767\nIteration  715 => Loss:40.571400\nIteration  716 => Loss:40.527233\nIteration  717 => Loss:40.483267\nIteration  718 => Loss:40.439500\nIteration  719 => Loss:40.395933\nIteration  720 => Loss:40.352567\nIteration  721 => Loss:40.309400\nIteration  722 => Loss:40.266433\nIteration  723 => Loss:40.223667\nIteration  724 => Loss:40.181100\nIteration  725 => Loss:40.138733\nIteration  726 => Loss:40.096567\nIteration  727 => Loss:40.054600\nIteration  728 => Loss:40.012833\nIteration  729 => Loss:39.971267\nIteration  730 => Loss:39.971147\nIteration  731 => Loss:39.927247\nIteration  732 => Loss:39.883547\nIteration  733 => Loss:39.840047\nIteration  734 => Loss:39.796747\nIteration  735 => Loss:39.753647\nIteration  736 => Loss:39.710747\nIteration  737 => Loss:39.668047\nIteration  738 => Loss:39.625547\nIteration  739 => Loss:39.583247\nIteration  740 => Loss:39.541147\nIteration  741 => Loss:39.499247\nIteration  742 => Loss:39.457547\nIteration  743 => Loss:39.416047\nIteration  744 => Loss:39.374747\nIteration  745 => Loss:39.333647\nIteration  746 => Loss:39.292747\nIteration  747 => Loss:39.252047\nIteration  748 => Loss:39.211547\nIteration  749 => Loss:39.209587\nIteration  750 => Loss:39.166753\nIteration  751 => Loss:39.124120\nIteration  752 => Loss:39.081687\nIteration  753 => Loss:39.039453\nIteration  754 => Loss:38.997420\nIteration  755 => Loss:38.955587\nIteration  756 => Loss:38.913953\nIteration  757 => Loss:38.872520\nIteration  758 => Loss:38.831287\nIteration  759 => Loss:38.790253\nIteration  760 => Loss:38.749420\nIteration  761 => Loss:38.708787\nIteration  762 => Loss:38.668353\nIteration  763 => Loss:38.628120\nIteration  764 => Loss:38.588087\nIteration  765 => Loss:38.548253\nIteration  766 => Loss:38.508620\nIteration  767 => Loss:38.507353\nIteration  768 => Loss:38.465387\nIteration  769 => Loss:38.423620\nIteration  770 => Loss:38.382053\nIteration  771 => Loss:38.340687\nIteration  772 => Loss:38.299520\nIteration  773 => Loss:38.258553\nIteration  774 => Loss:38.217787\nIteration  775 => Loss:38.177220\nIteration  776 => Loss:38.136853\nIteration  777 => Loss:38.096687\nIteration  778 => Loss:38.056720\nIteration  779 => Loss:38.016953\nIteration  780 => Loss:37.977387\nIteration  781 => Loss:37.938020\nIteration  782 => Loss:37.898853\nIteration  783 => Loss:37.859887\nIteration  784 => Loss:37.821120\nIteration  785 => Loss:37.820547\nIteration  786 => Loss:37.779447\nIteration  787 => Loss:37.738547\nIteration  788 => Loss:37.697847\nIteration  789 => Loss:37.657347\nIteration  790 => Loss:37.617047\nIteration  791 => Loss:37.576947\nIteration  792 => Loss:37.537047\nIteration  793 => Loss:37.497347\nIteration  794 => Loss:37.457847\nIteration  795 => Loss:37.418547\nIteration  796 => Loss:37.379447\nIteration  797 => Loss:37.340547\nIteration  798 => Loss:37.301847\nIteration  799 => Loss:37.263347\nIteration  800 => Loss:37.225047\nIteration  801 => Loss:37.186947\nIteration  802 => Loss:37.149047\nIteration  803 => Loss:37.111347\nIteration  804 => Loss:37.108933\nIteration  805 => Loss:37.068900\nIteration  806 => Loss:37.029067\nIteration  807 => Loss:36.989433\nIteration  808 => Loss:36.950000\nIteration  809 => Loss:36.910767\nIteration  810 => Loss:36.871733\nIteration  811 => Loss:36.832900\nIteration  812 => Loss:36.794267\nIteration  813 => Loss:36.755833\nIteration  814 => Loss:36.717600\nIteration  815 => Loss:36.679567\nIteration  816 => Loss:36.641733\nIteration  817 => Loss:36.604100\nIteration  818 => Loss:36.566667\nIteration  819 => Loss:36.529433\nIteration  820 => Loss:36.492400\nIteration  821 => Loss:36.455567\nIteration  822 => Loss:36.453847\nIteration  823 => Loss:36.414680\nIteration  824 => Loss:36.375713\nIteration  825 => Loss:36.336947\nIteration  826 => Loss:36.298380\nIteration  827 => Loss:36.260013\nIteration  828 => Loss:36.221847\nIteration  829 => Loss:36.183880\nIteration  830 => Loss:36.146113\nIteration  831 => Loss:36.108547\nIteration  832 => Loss:36.071180\nIteration  833 => Loss:36.034013\nIteration  834 => Loss:35.997047\nIteration  835 => Loss:35.960280\nIteration  836 => Loss:35.923713\nIteration  837 => Loss:35.887347\nIteration  838 => Loss:35.851180\nIteration  839 => Loss:35.815213\nIteration  840 => Loss:35.814187\nIteration  841 => Loss:35.775887\nIteration  842 => Loss:35.737787\nIteration  843 => Loss:35.699887\nIteration  844 => Loss:35.662187\nIteration  845 => Loss:35.624687\nIteration  846 => Loss:35.587387\nIteration  847 => Loss:35.550287\nIteration  848 => Loss:35.513387\nIteration  849 => Loss:35.476687\nIteration  850 => Loss:35.440187\nIteration  851 => Loss:35.403887\nIteration  852 => Loss:35.367787\nIteration  853 => Loss:35.331887\nIteration  854 => Loss:35.296187\nIteration  855 => Loss:35.260687\nIteration  856 => Loss:35.225387\nIteration  857 => Loss:35.190287\nIteration  858 => Loss:35.189953\nIteration  859 => Loss:35.152520\nIteration  860 => Loss:35.115287\nIteration  861 => Loss:35.078253\nIteration  862 => Loss:35.041420\nIteration  863 => Loss:35.004787\nIteration  864 => Loss:34.968353\nIteration  865 => Loss:34.932120\nIteration  866 => Loss:34.896087\nIteration  867 => Loss:34.860253\nIteration  868 => Loss:34.824620\nIteration  869 => Loss:34.789187\nIteration  870 => Loss:34.753953\nIteration  871 => Loss:34.718920\nIteration  872 => Loss:34.684087\nIteration  873 => Loss:34.649453\nIteration  874 => Loss:34.615020\nIteration  875 => Loss:34.580787\nIteration  876 => Loss:34.546753\nIteration  877 => Loss:34.544580\nIteration  878 => Loss:34.508213\nIteration  879 => Loss:34.472047\nIteration  880 => Loss:34.436080\nIteration  881 => Loss:34.400313\nIteration  882 => Loss:34.364747\nIteration  883 => Loss:34.329380\nIteration  884 => Loss:34.294213\nIteration  885 => Loss:34.259247\nIteration  886 => Loss:34.224480\nIteration  887 => Loss:34.189913\nIteration  888 => Loss:34.155547\nIteration  889 => Loss:34.121380\nIteration  890 => Loss:34.087413\nIteration  891 => Loss:34.053647\nIteration  892 => Loss:34.020080\nIteration  893 => Loss:33.986713\nIteration  894 => Loss:33.953547\nIteration  895 => Loss:33.952067\nIteration  896 => Loss:33.916567\nIteration  897 => Loss:33.881267\nIteration  898 => Loss:33.846167\nIteration  899 => Loss:33.811267\nIteration  900 => Loss:33.776567\nIteration  901 => Loss:33.742067\nIteration  902 => Loss:33.707767\nIteration  903 => Loss:33.673667\nIteration  904 => Loss:33.639767\nIteration  905 => Loss:33.606067\nIteration  906 => Loss:33.572567\nIteration  907 => Loss:33.539267\nIteration  908 => Loss:33.506167\nIteration  909 => Loss:33.473267\nIteration  910 => Loss:33.440567\nIteration  911 => Loss:33.408067\nIteration  912 => Loss:33.375767\nIteration  913 => Loss:33.374980\nIteration  914 => Loss:33.340347\nIteration  915 => Loss:33.305913\nIteration  916 => Loss:33.271680\nIteration  917 => Loss:33.237647\nIteration  918 => Loss:33.203813\nIteration  919 => Loss:33.170180\nIteration  920 => Loss:33.136747\nIteration  921 => Loss:33.103513\nIteration  922 => Loss:33.070480\nIteration  923 => Loss:33.037647\nIteration  924 => Loss:33.005013\nIteration  925 => Loss:32.972580\nIteration  926 => Loss:32.940347\nIteration  927 => Loss:32.908313\nIteration  928 => Loss:32.876480\nIteration  929 => Loss:32.844847\nIteration  930 => Loss:32.813413\nIteration  931 => Loss:32.813320\nIteration  932 => Loss:32.779553\nIteration  933 => Loss:32.745987\nIteration  934 => Loss:32.712620\nIteration  935 => Loss:32.679453\nIteration  936 => Loss:32.646487\nIteration  937 => Loss:32.613720\nIteration  938 => Loss:32.581153\nIteration  939 => Loss:32.548787\nIteration  940 => Loss:32.516620\nIteration  941 => Loss:32.484653\nIteration  942 => Loss:32.452887\nIteration  943 => Loss:32.421320\nIteration  944 => Loss:32.389953\nIteration  945 => Loss:32.358787\nIteration  946 => Loss:32.327820\nIteration  947 => Loss:32.297053\nIteration  948 => Loss:32.266487\nIteration  949 => Loss:32.236120\nIteration  950 => Loss:32.234187\nIteration  951 => Loss:32.201487\nIteration  952 => Loss:32.168987\nIteration  953 => Loss:32.136687\nIteration  954 => Loss:32.104587\nIteration  955 => Loss:32.072687\nIteration  956 => Loss:32.040987\nIteration  957 => Loss:32.009487\nIteration  958 => Loss:31.978187\nIteration  959 => Loss:31.947087\nIteration  960 => Loss:31.916187\nIteration  961 => Loss:31.885487\nIteration  962 => Loss:31.854987\nIteration  963 => Loss:31.824687\nIteration  964 => Loss:31.794587\nIteration  965 => Loss:31.764687\nIteration  966 => Loss:31.734987\nIteration  967 => Loss:31.705487\nIteration  968 => Loss:31.704247\nIteration  969 => Loss:31.672413\nIteration  970 => Loss:31.640780\nIteration  971 => Loss:31.609347\nIteration  972 => Loss:31.578113\nIteration  973 => Loss:31.547080\nIteration  974 => Loss:31.516247\nIteration  975 => Loss:31.485613\nIteration  976 => Loss:31.455180\nIteration  977 => Loss:31.424947\nIteration  978 => Loss:31.394913\nIteration  979 => Loss:31.365080\nIteration  980 => Loss:31.335447\nIteration  981 => Loss:31.306013\nIteration  982 => Loss:31.276780\nIteration  983 => Loss:31.247747\nIteration  984 => Loss:31.218913\nIteration  985 => Loss:31.190280\nIteration  986 => Loss:31.189733\nIteration  987 => Loss:31.158767\nIteration  988 => Loss:31.128000\nIteration  989 => Loss:31.097433\nIteration  990 => Loss:31.067067\nIteration  991 => Loss:31.036900\nIteration  992 => Loss:31.006933\nIteration  993 => Loss:30.977167\nIteration  994 => Loss:30.947600\nIteration  995 => Loss:30.918233\nIteration  996 => Loss:30.889067\nIteration  997 => Loss:30.860100\nIteration  998 => Loss:30.831333\nIteration  999 => Loss:30.802767\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-311ac952313a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, iterations = 1000, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducting gradient descent, keeping bias fixed for simplicity at the moment \n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.average(X * (predict(X, w, 0) - Y))\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = 0\n",
    "    for i in range(iterations):\n",
    "        print(\"Iterations %4d => Loss: %.10f\" %(i, loss(X , Y, w, 0)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iterations    0 => Loss: 812.8666666667\nIterations    1 => Loss: 304.3630879787\nIterations    2 => Loss: 143.5265791020\nIterations    3 => Loss: 92.6549949641\nIterations    4 => Loss: 76.5646303400\nIterations    5 => Loss: 71.4753484132\nIterations    6 => Loss: 69.8656402969\nIterations    7 => Loss: 69.3564996643\nIterations    8 => Loss: 69.1954616593\nIterations    9 => Loss: 69.1445263431\nIterations   10 => Loss: 69.1284158205\nIterations   11 => Loss: 69.1233201627\nIterations   12 => Loss: 69.1217084379\nIterations   13 => Loss: 69.1211986595\nIterations   14 => Loss: 69.1210374197\nIterations   15 => Loss: 69.1209864206\nIterations   16 => Loss: 69.1209702899\nIterations   17 => Loss: 69.1209651878\nIterations   18 => Loss: 69.1209635741\nIterations   19 => Loss: 69.1209630637\nIterations   20 => Loss: 69.1209629022\nIterations   21 => Loss: 69.1209628512\nIterations   22 => Loss: 69.1209628350\nIterations   23 => Loss: 69.1209628299\nIterations   24 => Loss: 69.1209628283\nIterations   25 => Loss: 69.1209628278\nIterations   26 => Loss: 69.1209628276\nIterations   27 => Loss: 69.1209628276\nIterations   28 => Loss: 69.1209628276\nIterations   29 => Loss: 69.1209628275\nIterations   30 => Loss: 69.1209628275\nIterations   31 => Loss: 69.1209628275\nIterations   32 => Loss: 69.1209628275\nIterations   33 => Loss: 69.1209628275\nIterations   34 => Loss: 69.1209628275\nIterations   35 => Loss: 69.1209628275\nIterations   36 => Loss: 69.1209628275\nIterations   37 => Loss: 69.1209628275\nIterations   38 => Loss: 69.1209628275\nIterations   39 => Loss: 69.1209628275\nIterations   40 => Loss: 69.1209628275\nIterations   41 => Loss: 69.1209628275\nIterations   42 => Loss: 69.1209628275\nIterations   43 => Loss: 69.1209628275\nIterations   44 => Loss: 69.1209628275\nIterations   45 => Loss: 69.1209628275\nIterations   46 => Loss: 69.1209628275\nIterations   47 => Loss: 69.1209628275\nIterations   48 => Loss: 69.1209628275\nIterations   49 => Loss: 69.1209628275\nIterations   50 => Loss: 69.1209628275\nIterations   51 => Loss: 69.1209628275\nIterations   52 => Loss: 69.1209628275\nIterations   53 => Loss: 69.1209628275\nIterations   54 => Loss: 69.1209628275\nIterations   55 => Loss: 69.1209628275\nIterations   56 => Loss: 69.1209628275\nIterations   57 => Loss: 69.1209628275\nIterations   58 => Loss: 69.1209628275\nIterations   59 => Loss: 69.1209628275\nIterations   60 => Loss: 69.1209628275\nIterations   61 => Loss: 69.1209628275\nIterations   62 => Loss: 69.1209628275\nIterations   63 => Loss: 69.1209628275\nIterations   64 => Loss: 69.1209628275\nIterations   65 => Loss: 69.1209628275\nIterations   66 => Loss: 69.1209628275\nIterations   67 => Loss: 69.1209628275\nIterations   68 => Loss: 69.1209628275\nIterations   69 => Loss: 69.1209628275\nIterations   70 => Loss: 69.1209628275\nIterations   71 => Loss: 69.1209628275\nIterations   72 => Loss: 69.1209628275\nIterations   73 => Loss: 69.1209628275\nIterations   74 => Loss: 69.1209628275\nIterations   75 => Loss: 69.1209628275\nIterations   76 => Loss: 69.1209628275\nIterations   77 => Loss: 69.1209628275\nIterations   78 => Loss: 69.1209628275\nIterations   79 => Loss: 69.1209628275\nIterations   80 => Loss: 69.1209628275\nIterations   81 => Loss: 69.1209628275\nIterations   82 => Loss: 69.1209628275\nIterations   83 => Loss: 69.1209628275\nIterations   84 => Loss: 69.1209628275\nIterations   85 => Loss: 69.1209628275\nIterations   86 => Loss: 69.1209628275\nIterations   87 => Loss: 69.1209628275\nIterations   88 => Loss: 69.1209628275\nIterations   89 => Loss: 69.1209628275\nIterations   90 => Loss: 69.1209628275\nIterations   91 => Loss: 69.1209628275\nIterations   92 => Loss: 69.1209628275\nIterations   93 => Loss: 69.1209628275\nIterations   94 => Loss: 69.1209628275\nIterations   95 => Loss: 69.1209628275\nIterations   96 => Loss: 69.1209628275\nIterations   97 => Loss: 69.1209628275\nIterations   98 => Loss: 69.1209628275\nIterations   99 => Loss: 69.1209628275\n\nw=1.8436928702\n"
     ]
    }
   ],
   "source": [
    "w = train(X, Y, iterations = 100, lr=0.001)\n",
    "print(\"\\nw=%.10f\" % w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're looking at multi D gradient Descent\n",
    "def predict(X, w, b):\n",
    "    return X * w + b\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "def gradient(X, Y, w, b):\n",
    "    w_gradient = 2 * np.average(X * (predict(X, w, b) - Y))\n",
    "    b_gradient = 2 * np.average((predict(X, w, b) - Y))\n",
    "    return (w_gradient, b_gradient)\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        print(\"Iterations %4d => Loss: %.10f\" %(i, loss(X , Y, w, b)))\n",
    "        w_gradient, b_gradient = gradient(X, Y, w, b)\n",
    "        w -= w_gradient * lr\n",
    "        b -= b_gradient * lr\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".7123701772\n",
      "Iterations 1489 => Loss: 61.7082374291\n",
      "Iterations 1490 => Loss: 61.7041051203\n",
      "Iterations 1491 => Loss: 61.6999732509\n",
      "Iterations 1492 => Loss: 61.6958418207\n",
      "Iterations 1493 => Loss: 61.6917108299\n",
      "Iterations 1494 => Loss: 61.6875802783\n",
      "Iterations 1495 => Loss: 61.6834501658\n",
      "Iterations 1496 => Loss: 61.6793204925\n",
      "Iterations 1497 => Loss: 61.6751912583\n",
      "Iterations 1498 => Loss: 61.6710624631\n",
      "Iterations 1499 => Loss: 61.6669341069\n",
      "Iterations 1500 => Loss: 61.6628061896\n",
      "Iterations 1501 => Loss: 61.6586787112\n",
      "Iterations 1502 => Loss: 61.6545516717\n",
      "Iterations 1503 => Loss: 61.6504250709\n",
      "Iterations 1504 => Loss: 61.6462989090\n",
      "Iterations 1505 => Loss: 61.6421731857\n",
      "Iterations 1506 => Loss: 61.6380479011\n",
      "Iterations 1507 => Loss: 61.6339230551\n",
      "Iterations 1508 => Loss: 61.6297986477\n",
      "Iterations 1509 => Loss: 61.6256746787\n",
      "Iterations 1510 => Loss: 61.6215511483\n",
      "Iterations 1511 => Loss: 61.6174280563\n",
      "Iterations 1512 => Loss: 61.6133054027\n",
      "Iterations 1513 => Loss: 61.6091831874\n",
      "Iterations 1514 => Loss: 61.6050614104\n",
      "Iterations 1515 => Loss: 61.6009400716\n",
      "Iterations 1516 => Loss: 61.5968191710\n",
      "Iterations 1517 => Loss: 61.5926987086\n",
      "Iterations 1518 => Loss: 61.5885786843\n",
      "Iterations 1519 => Loss: 61.5844590980\n",
      "Iterations 1520 => Loss: 61.5803399498\n",
      "Iterations 1521 => Loss: 61.5762212395\n",
      "Iterations 1522 => Loss: 61.5721029671\n",
      "Iterations 1523 => Loss: 61.5679851326\n",
      "Iterations 1524 => Loss: 61.5638677359\n",
      "Iterations 1525 => Loss: 61.5597507770\n",
      "Iterations 1526 => Loss: 61.5556342558\n",
      "Iterations 1527 => Loss: 61.5515181723\n",
      "Iterations 1528 => Loss: 61.5474025264\n",
      "Iterations 1529 => Loss: 61.5432873181\n",
      "Iterations 1530 => Loss: 61.5391725474\n",
      "Iterations 1531 => Loss: 61.5350582141\n",
      "Iterations 1532 => Loss: 61.5309443183\n",
      "Iterations 1533 => Loss: 61.5268308599\n",
      "Iterations 1534 => Loss: 61.5227178389\n",
      "Iterations 1535 => Loss: 61.5186052552\n",
      "Iterations 1536 => Loss: 61.5144931087\n",
      "Iterations 1537 => Loss: 61.5103813995\n",
      "Iterations 1538 => Loss: 61.5062701274\n",
      "Iterations 1539 => Loss: 61.5021592925\n",
      "Iterations 1540 => Loss: 61.4980488946\n",
      "Iterations 1541 => Loss: 61.4939389338\n",
      "Iterations 1542 => Loss: 61.4898294099\n",
      "Iterations 1543 => Loss: 61.4857203230\n",
      "Iterations 1544 => Loss: 61.4816116730\n",
      "Iterations 1545 => Loss: 61.4775034598\n",
      "Iterations 1546 => Loss: 61.4733956834\n",
      "Iterations 1547 => Loss: 61.4692883438\n",
      "Iterations 1548 => Loss: 61.4651814408\n",
      "Iterations 1549 => Loss: 61.4610749746\n",
      "Iterations 1550 => Loss: 61.4569689449\n",
      "Iterations 1551 => Loss: 61.4528633518\n",
      "Iterations 1552 => Loss: 61.4487581953\n",
      "Iterations 1553 => Loss: 61.4446534752\n",
      "Iterations 1554 => Loss: 61.4405491915\n",
      "Iterations 1555 => Loss: 61.4364453442\n",
      "Iterations 1556 => Loss: 61.4323419333\n",
      "Iterations 1557 => Loss: 61.4282389586\n",
      "Iterations 1558 => Loss: 61.4241364202\n",
      "Iterations 1559 => Loss: 61.4200343180\n",
      "Iterations 1560 => Loss: 61.4159326519\n",
      "Iterations 1561 => Loss: 61.4118314220\n",
      "Iterations 1562 => Loss: 61.4077306281\n",
      "Iterations 1563 => Loss: 61.4036302702\n",
      "Iterations 1564 => Loss: 61.3995303482\n",
      "Iterations 1565 => Loss: 61.3954308622\n",
      "Iterations 1566 => Loss: 61.3913318120\n",
      "Iterations 1567 => Loss: 61.3872331977\n",
      "Iterations 1568 => Loss: 61.3831350192\n",
      "Iterations 1569 => Loss: 61.3790372763\n",
      "Iterations 1570 => Loss: 61.3749399692\n",
      "Iterations 1571 => Loss: 61.3708430977\n",
      "Iterations 1572 => Loss: 61.3667466618\n",
      "Iterations 1573 => Loss: 61.3626506615\n",
      "Iterations 1574 => Loss: 61.3585550966\n",
      "Iterations 1575 => Loss: 61.3544599672\n",
      "Iterations 1576 => Loss: 61.3503652732\n",
      "Iterations 1577 => Loss: 61.3462710146\n",
      "Iterations 1578 => Loss: 61.3421771913\n",
      "Iterations 1579 => Loss: 61.3380838033\n",
      "Iterations 1580 => Loss: 61.3339908504\n",
      "Iterations 1581 => Loss: 61.3298983328\n",
      "Iterations 1582 => Loss: 61.3258062503\n",
      "Iterations 1583 => Loss: 61.3217146028\n",
      "Iterations 1584 => Loss: 61.3176233904\n",
      "Iterations 1585 => Loss: 61.3135326130\n",
      "Iterations 1586 => Loss: 61.3094422706\n",
      "Iterations 1587 => Loss: 61.3053523630\n",
      "Iterations 1588 => Loss: 61.3012628903\n",
      "Iterations 1589 => Loss: 61.2971738524\n",
      "Iterations 1590 => Loss: 61.2930852493\n",
      "Iterations 1591 => Loss: 61.2889970809\n",
      "Iterations 1592 => Loss: 61.2849093471\n",
      "Iterations 1593 => Loss: 61.2808220480\n",
      "Iterations 1594 => Loss: 61.2767351834\n",
      "Iterations 1595 => Loss: 61.2726487534\n",
      "Iterations 1596 => Loss: 61.2685627578\n",
      "Iterations 1597 => Loss: 61.2644771967\n",
      "Iterations 1598 => Loss: 61.2603920700\n",
      "Iterations 1599 => Loss: 61.2563073776\n",
      "Iterations 1600 => Loss: 61.2522231195\n",
      "Iterations 1601 => Loss: 61.2481392957\n",
      "Iterations 1602 => Loss: 61.2440559061\n",
      "Iterations 1603 => Loss: 61.2399729506\n",
      "Iterations 1604 => Loss: 61.2358904293\n",
      "Iterations 1605 => Loss: 61.2318083420\n",
      "Iterations 1606 => Loss: 61.2277266887\n",
      "Iterations 1607 => Loss: 61.2236454694\n",
      "Iterations 1608 => Loss: 61.2195646841\n",
      "Iterations 1609 => Loss: 61.2154843326\n",
      "Iterations 1610 => Loss: 61.2114044149\n",
      "Iterations 1611 => Loss: 61.2073249311\n",
      "Iterations 1612 => Loss: 61.2032458810\n",
      "Iterations 1613 => Loss: 61.1991672646\n",
      "Iterations 1614 => Loss: 61.1950890818\n",
      "Iterations 1615 => Loss: 61.1910113327\n",
      "Iterations 1616 => Loss: 61.1869340171\n",
      "Iterations 1617 => Loss: 61.1828571350\n",
      "Iterations 1618 => Loss: 61.1787806864\n",
      "Iterations 1619 => Loss: 61.1747046712\n",
      "Iterations 1620 => Loss: 61.1706290894\n",
      "Iterations 1621 => Loss: 61.1665539409\n",
      "Iterations 1622 => Loss: 61.1624792257\n",
      "Iterations 1623 => Loss: 61.1584049438\n",
      "Iterations 1624 => Loss: 61.1543310950\n",
      "Iterations 1625 => Loss: 61.1502576794\n",
      "Iterations 1626 => Loss: 61.1461846968\n",
      "Iterations 1627 => Loss: 61.1421121474\n",
      "Iterations 1628 => Loss: 61.1380400309\n",
      "Iterations 1629 => Loss: 61.1339683474\n",
      "Iterations 1630 => Loss: 61.1298970968\n",
      "Iterations 1631 => Loss: 61.1258262791\n",
      "Iterations 1632 => Loss: 61.1217558942\n",
      "Iterations 1633 => Loss: 61.1176859421\n",
      "Iterations 1634 => Loss: 61.1136164227\n",
      "Iterations 1635 => Loss: 61.1095473360\n",
      "Iterations 1636 => Loss: 61.1054786819\n",
      "Iterations 1637 => Loss: 61.1014104604\n",
      "Iterations 1638 => Loss: 61.0973426715\n",
      "Iterations 1639 => Loss: 61.0932753151\n",
      "Iterations 1640 => Loss: 61.0892083911\n",
      "Iterations 1641 => Loss: 61.0851418995\n",
      "Iterations 1642 => Loss: 61.0810758403\n",
      "Iterations 1643 => Loss: 61.0770102135\n",
      "Iterations 1644 => Loss: 61.0729450188\n",
      "Iterations 1645 => Loss: 61.0688802565\n",
      "Iterations 1646 => Loss: 61.0648159262\n",
      "Iterations 1647 => Loss: 61.0607520282\n",
      "Iterations 1648 => Loss: 61.0566885622\n",
      "Iterations 1649 => Loss: 61.0526255282\n",
      "Iterations 1650 => Loss: 61.0485629263\n",
      "Iterations 1651 => Loss: 61.0445007563\n",
      "Iterations 1652 => Loss: 61.0404390182\n",
      "Iterations 1653 => Loss: 61.0363777119\n",
      "Iterations 1654 => Loss: 61.0323168375\n",
      "Iterations 1655 => Loss: 61.0282563948\n",
      "Iterations 1656 => Loss: 61.0241963839\n",
      "Iterations 1657 => Loss: 61.0201368046\n",
      "Iterations 1658 => Loss: 61.0160776570\n",
      "Iterations 1659 => Loss: 61.0120189409\n",
      "Iterations 1660 => Loss: 61.0079606564\n",
      "Iterations 1661 => Loss: 61.0039028034\n",
      "Iterations 1662 => Loss: 60.9998453818\n",
      "Iterations 1663 => Loss: 60.9957883916\n",
      "Iterations 1664 => Loss: 60.9917318327\n",
      "Iterations 1665 => Loss: 60.9876757052\n",
      "Iterations 1666 => Loss: 60.9836200089\n",
      "Iterations 1667 => Loss: 60.9795647439\n",
      "Iterations 1668 => Loss: 60.9755099100\n",
      "Iterations 1669 => Loss: 60.9714555072\n",
      "Iterations 1670 => Loss: 60.9674015355\n",
      "Iterations 1671 => Loss: 60.9633479949\n",
      "Iterations 1672 => Loss: 60.9592948852\n",
      "Iterations 1673 => Loss: 60.9552422065\n",
      "Iterations 1674 => Loss: 60.9511899587\n",
      "Iterations 1675 => Loss: 60.9471381417\n",
      "Iterations 1676 => Loss: 60.9430867555\n",
      "Iterations 1677 => Loss: 60.9390358001\n",
      "Iterations 1678 => Loss: 60.9349852753\n",
      "Iterations 1679 => Loss: 60.9309351813\n",
      "Iterations 1680 => Loss: 60.9268855179\n",
      "Iterations 1681 => Loss: 60.9228362850\n",
      "Iterations 1682 => Loss: 60.9187874827\n",
      "Iterations 1683 => Loss: 60.9147391108\n",
      "Iterations 1684 => Loss: 60.9106911694\n",
      "Iterations 1685 => Loss: 60.9066436584\n",
      "Iterations 1686 => Loss: 60.9025965777\n",
      "Iterations 1687 => Loss: 60.8985499273\n",
      "Iterations 1688 => Loss: 60.8945037072\n",
      "Iterations 1689 => Loss: 60.8904579173\n",
      "Iterations 1690 => Loss: 60.8864125575\n",
      "Iterations 1691 => Loss: 60.8823676279\n",
      "Iterations 1692 => Loss: 60.8783231283\n",
      "Iterations 1693 => Loss: 60.8742790588\n",
      "Iterations 1694 => Loss: 60.8702354192\n",
      "Iterations 1695 => Loss: 60.8661922096\n",
      "Iterations 1696 => Loss: 60.8621494299\n",
      "Iterations 1697 => Loss: 60.8581070800\n",
      "Iterations 1698 => Loss: 60.8540651598\n",
      "Iterations 1699 => Loss: 60.8500236695\n",
      "Iterations 1700 => Loss: 60.8459826088\n",
      "Iterations 1701 => Loss: 60.8419419778\n",
      "Iterations 1702 => Loss: 60.8379017765\n",
      "Iterations 1703 => Loss: 60.8338620047\n",
      "Iterations 1704 => Loss: 60.8298226624\n",
      "Iterations 1705 => Loss: 60.8257837496\n",
      "Iterations 1706 => Loss: 60.8217452662\n",
      "Iterations 1707 => Loss: 60.8177072122\n",
      "Iterations 1708 => Loss: 60.8136695875\n",
      "Iterations 1709 => Loss: 60.8096323921\n",
      "Iterations 1710 => Loss: 60.8055956260\n",
      "Iterations 1711 => Loss: 60.8015592891\n",
      "Iterations 1712 => Loss: 60.7975233814\n",
      "Iterations 1713 => Loss: 60.7934879027\n",
      "Iterations 1714 => Loss: 60.7894528531\n",
      "Iterations 1715 => Loss: 60.7854182326\n",
      "Iterations 1716 => Loss: 60.7813840410\n",
      "Iterations 1717 => Loss: 60.7773502783\n",
      "Iterations 1718 => Loss: 60.7733169445\n",
      "Iterations 1719 => Loss: 60.7692840396\n",
      "Iterations 1720 => Loss: 60.7652515635\n",
      "Iterations 1721 => Loss: 60.7612195161\n",
      "Iterations 1722 => Loss: 60.7571878974\n",
      "Iterations 1723 => Loss: 60.7531567073\n",
      "Iterations 1724 => Loss: 60.7491259459\n",
      "Iterations 1725 => Loss: 60.7450956130\n",
      "Iterations 1726 => Loss: 60.7410657087\n",
      "Iterations 1727 => Loss: 60.7370362328\n",
      "Iterations 1728 => Loss: 60.7330071853\n",
      "Iterations 1729 => Loss: 60.7289785663\n",
      "Iterations 1730 => Loss: 60.7249503755\n",
      "Iterations 1731 => Loss: 60.7209226131\n",
      "Iterations 1732 => Loss: 60.7168952789\n",
      "Iterations 1733 => Loss: 60.7128683729\n",
      "Iterations 1734 => Loss: 60.7088418950\n",
      "Iterations 1735 => Loss: 60.7048158453\n",
      "Iterations 1736 => Loss: 60.7007902236\n",
      "Iterations 1737 => Loss: 60.6967650300\n",
      "Iterations 1738 => Loss: 60.6927402643\n",
      "Iterations 1739 => Loss: 60.6887159265\n",
      "Iterations 1740 => Loss: 60.6846920166\n",
      "Iterations 1741 => Loss: 60.6806685346\n",
      "Iterations 1742 => Loss: 60.6766454803\n",
      "Iterations 1743 => Loss: 60.6726228538\n",
      "Iterations 1744 => Loss: 60.6686006550\n",
      "Iterations 1745 => Loss: 60.6645788839\n",
      "Iterations 1746 => Loss: 60.6605575403\n",
      "Iterations 1747 => Loss: 60.6565366243\n",
      "Iterations 1748 => Loss: 60.6525161359\n",
      "Iterations 1749 => Loss: 60.6484960749\n",
      "Iterations 1750 => Loss: 60.6444764413\n",
      "Iterations 1751 => Loss: 60.6404572351\n",
      "Iterations 1752 => Loss: 60.6364384563\n",
      "Iterations 1753 => Loss: 60.6324201047\n",
      "Iterations 1754 => Loss: 60.6284021804\n",
      "Iterations 1755 => Loss: 60.6243846833\n",
      "Iterations 1756 => Loss: 60.6203676133\n",
      "Iterations 1757 => Loss: 60.6163509704\n",
      "Iterations 1758 => Loss: 60.6123347546\n",
      "Iterations 1759 => Loss: 60.6083189659\n",
      "Iterations 1760 => Loss: 60.6043036041\n",
      "Iterations 1761 => Loss: 60.6002886692\n",
      "Iterations 1762 => Loss: 60.5962741612\n",
      "Iterations 1763 => Loss: 60.5922600800\n",
      "Iterations 1764 => Loss: 60.5882464256\n",
      "Iterations 1765 => Loss: 60.5842331980\n",
      "Iterations 1766 => Loss: 60.5802203971\n",
      "Iterations 1767 => Loss: 60.5762080228\n",
      "Iterations 1768 => Loss: 60.5721960751\n",
      "Iterations 1769 => Loss: 60.5681845540\n",
      "Iterations 1770 => Loss: 60.5641734595\n",
      "Iterations 1771 => Loss: 60.5601627914\n",
      "Iterations 1772 => Loss: 60.5561525497\n",
      "Iterations 1773 => Loss: 60.5521427344\n",
      "Iterations 1774 => Loss: 60.5481333454\n",
      "Iterations 1775 => Loss: 60.5441243827\n",
      "Iterations 1776 => Loss: 60.5401158463\n",
      "Iterations 1777 => Loss: 60.5361077361\n",
      "Iterations 1778 => Loss: 60.5321000520\n",
      "Iterations 1779 => Loss: 60.5280927941\n",
      "Iterations 1780 => Loss: 60.5240859622\n",
      "Iterations 1781 => Loss: 60.5200795563\n",
      "Iterations 1782 => Loss: 60.5160735764\n",
      "Iterations 1783 => Loss: 60.5120680224\n",
      "Iterations 1784 => Loss: 60.5080628943\n",
      "Iterations 1785 => Loss: 60.5040581921\n",
      "Iterations 1786 => Loss: 60.5000539156\n",
      "Iterations 1787 => Loss: 60.4960500649\n",
      "Iterations 1788 => Loss: 60.4920466399\n",
      "Iterations 1789 => Loss: 60.4880436406\n",
      "Iterations 1790 => Loss: 60.4840410668\n",
      "Iterations 1791 => Loss: 60.4800389187\n",
      "Iterations 1792 => Loss: 60.4760371960\n",
      "Iterations 1793 => Loss: 60.4720358988\n",
      "Iterations 1794 => Loss: 60.4680350271\n",
      "Iterations 1795 => Loss: 60.4640345807\n",
      "Iterations 1796 => Loss: 60.4600345597\n",
      "Iterations 1797 => Loss: 60.4560349640\n",
      "Iterations 1798 => Loss: 60.4520357935\n",
      "Iterations 1799 => Loss: 60.4480370483\n",
      "Iterations 1800 => Loss: 60.4440387282\n",
      "Iterations 1801 => Loss: 60.4400408332\n",
      "Iterations 1802 => Loss: 60.4360433633\n",
      "Iterations 1803 => Loss: 60.4320463184\n",
      "Iterations 1804 => Loss: 60.4280496985\n",
      "Iterations 1805 => Loss: 60.4240535035\n",
      "Iterations 1806 => Loss: 60.4200577334\n",
      "Iterations 1807 => Loss: 60.4160623881\n",
      "Iterations 1808 => Loss: 60.4120674677\n",
      "Iterations 1809 => Loss: 60.4080729720\n",
      "Iterations 1810 => Loss: 60.4040789010\n",
      "Iterations 1811 => Loss: 60.4000852547\n",
      "Iterations 1812 => Loss: 60.3960920330\n",
      "Iterations 1813 => Loss: 60.3920992358\n",
      "Iterations 1814 => Loss: 60.3881068632\n",
      "Iterations 1815 => Loss: 60.3841149151\n",
      "Iterations 1816 => Loss: 60.3801233914\n",
      "Iterations 1817 => Loss: 60.3761322921\n",
      "Iterations 1818 => Loss: 60.3721416172\n",
      "Iterations 1819 => Loss: 60.3681513665\n",
      "Iterations 1820 => Loss: 60.3641615401\n",
      "Iterations 1821 => Loss: 60.3601721379\n",
      "Iterations 1822 => Loss: 60.3561831599\n",
      "Iterations 1823 => Loss: 60.3521946060\n",
      "Iterations 1824 => Loss: 60.3482064762\n",
      "Iterations 1825 => Loss: 60.3442187704\n",
      "Iterations 1826 => Loss: 60.3402314886\n",
      "Iterations 1827 => Loss: 60.3362446308\n",
      "Iterations 1828 => Loss: 60.3322581968\n",
      "Iterations 1829 => Loss: 60.3282721867\n",
      "Iterations 1830 => Loss: 60.3242866004\n",
      "Iterations 1831 => Loss: 60.3203014379\n",
      "Iterations 1832 => Loss: 60.3163166990\n",
      "Iterations 1833 => Loss: 60.3123323839\n",
      "Iterations 1834 => Loss: 60.3083484923\n",
      "Iterations 1835 => Loss: 60.3043650244\n",
      "Iterations 1836 => Loss: 60.3003819800\n",
      "Iterations 1837 => Loss: 60.2963993590\n",
      "Iterations 1838 => Loss: 60.2924171616\n",
      "Iterations 1839 => Loss: 60.2884353875\n",
      "Iterations 1840 => Loss: 60.2844540368\n",
      "Iterations 1841 => Loss: 60.2804731093\n",
      "Iterations 1842 => Loss: 60.2764926052\n",
      "Iterations 1843 => Loss: 60.2725125243\n",
      "Iterations 1844 => Loss: 60.2685328665\n",
      "Iterations 1845 => Loss: 60.2645536319\n",
      "Iterations 1846 => Loss: 60.2605748204\n",
      "Iterations 1847 => Loss: 60.2565964319\n",
      "Iterations 1848 => Loss: 60.2526184664\n",
      "Iterations 1849 => Loss: 60.2486409238\n",
      "Iterations 1850 => Loss: 60.2446638042\n",
      "Iterations 1851 => Loss: 60.2406871074\n",
      "Iterations 1852 => Loss: 60.2367108334\n",
      "Iterations 1853 => Loss: 60.2327349822\n",
      "Iterations 1854 => Loss: 60.2287595537\n",
      "Iterations 1855 => Loss: 60.2247845479\n",
      "Iterations 1856 => Loss: 60.2208099648\n",
      "Iterations 1857 => Loss: 60.2168358042\n",
      "Iterations 1858 => Loss: 60.2128620662\n",
      "Iterations 1859 => Loss: 60.2088887507\n",
      "Iterations 1860 => Loss: 60.2049158576\n",
      "Iterations 1861 => Loss: 60.2009433870\n",
      "Iterations 1862 => Loss: 60.1969713387\n",
      "Iterations 1863 => Loss: 60.1929997127\n",
      "Iterations 1864 => Loss: 60.1890285090\n",
      "Iterations 1865 => Loss: 60.1850577276\n",
      "Iterations 1866 => Loss: 60.1810873683\n",
      "Iterations 1867 => Loss: 60.1771174312\n",
      "Iterations 1868 => Loss: 60.1731479161\n",
      "Iterations 1869 => Loss: 60.1691788232\n",
      "Iterations 1870 => Loss: 60.1652101522\n",
      "Iterations 1871 => Loss: 60.1612419032\n",
      "Iterations 1872 => Loss: 60.1572740761\n",
      "Iterations 1873 => Loss: 60.1533066709\n",
      "Iterations 1874 => Loss: 60.1493396875\n",
      "Iterations 1875 => Loss: 60.1453731259\n",
      "Iterations 1876 => Loss: 60.1414069860\n",
      "Iterations 1877 => Loss: 60.1374412678\n",
      "Iterations 1878 => Loss: 60.1334759713\n",
      "Iterations 1879 => Loss: 60.1295110964\n",
      "Iterations 1880 => Loss: 60.1255466430\n",
      "Iterations 1881 => Loss: 60.1215826111\n",
      "Iterations 1882 => Loss: 60.1176190008\n",
      "Iterations 1883 => Loss: 60.1136558118\n",
      "Iterations 1884 => Loss: 60.1096930442\n",
      "Iterations 1885 => Loss: 60.1057306980\n",
      "Iterations 1886 => Loss: 60.1017687730\n",
      "Iterations 1887 => Loss: 60.0978072693\n",
      "Iterations 1888 => Loss: 60.0938461868\n",
      "Iterations 1889 => Loss: 60.0898855254\n",
      "Iterations 1890 => Loss: 60.0859252852\n",
      "Iterations 1891 => Loss: 60.0819654660\n",
      "Iterations 1892 => Loss: 60.0780060678\n",
      "Iterations 1893 => Loss: 60.0740470907\n",
      "Iterations 1894 => Loss: 60.0700885344\n",
      "Iterations 1895 => Loss: 60.0661303990\n",
      "Iterations 1896 => Loss: 60.0621726845\n",
      "Iterations 1897 => Loss: 60.0582153908\n",
      "Iterations 1898 => Loss: 60.0542585178\n",
      "Iterations 1899 => Loss: 60.0503020656\n",
      "Iterations 1900 => Loss: 60.0463460340\n",
      "Iterations 1901 => Loss: 60.0423904230\n",
      "Iterations 1902 => Loss: 60.0384352326\n",
      "Iterations 1903 => Loss: 60.0344804627\n",
      "Iterations 1904 => Loss: 60.0305261133\n",
      "Iterations 1905 => Loss: 60.0265721843\n",
      "Iterations 1906 => Loss: 60.0226186758\n",
      "Iterations 1907 => Loss: 60.0186655876\n",
      "Iterations 1908 => Loss: 60.0147129197\n",
      "Iterations 1909 => Loss: 60.0107606720\n",
      "Iterations 1910 => Loss: 60.0068088446\n",
      "Iterations 1911 => Loss: 60.0028574373\n",
      "Iterations 1912 => Loss: 59.9989064502\n",
      "Iterations 1913 => Loss: 59.9949558831\n",
      "Iterations 1914 => Loss: 59.9910057361\n",
      "Iterations 1915 => Loss: 59.9870560091\n",
      "Iterations 1916 => Loss: 59.9831067020\n",
      "Iterations 1917 => Loss: 59.9791578149\n",
      "Iterations 1918 => Loss: 59.9752093476\n",
      "Iterations 1919 => Loss: 59.9712613001\n",
      "Iterations 1920 => Loss: 59.9673136723\n",
      "Iterations 1921 => Loss: 59.9633664643\n",
      "Iterations 1922 => Loss: 59.9594196760\n",
      "Iterations 1923 => Loss: 59.9554733073\n",
      "Iterations 1924 => Loss: 59.9515273583\n",
      "Iterations 1925 => Loss: 59.9475818287\n",
      "Iterations 1926 => Loss: 59.9436367187\n",
      "Iterations 1927 => Loss: 59.9396920281\n",
      "Iterations 1928 => Loss: 59.9357477569\n",
      "Iterations 1929 => Loss: 59.9318039051\n",
      "Iterations 1930 => Loss: 59.9278604726\n",
      "Iterations 1931 => Loss: 59.9239174594\n",
      "Iterations 1932 => Loss: 59.9199748654\n",
      "Iterations 1933 => Loss: 59.9160326906\n",
      "Iterations 1934 => Loss: 59.9120909350\n",
      "Iterations 1935 => Loss: 59.9081495985\n",
      "Iterations 1936 => Loss: 59.9042086810\n",
      "Iterations 1937 => Loss: 59.9002681825\n",
      "Iterations 1938 => Loss: 59.8963281030\n",
      "Iterations 1939 => Loss: 59.8923884424\n",
      "Iterations 1940 => Loss: 59.8884492007\n",
      "Iterations 1941 => Loss: 59.8845103779\n",
      "Iterations 1942 => Loss: 59.8805719738\n",
      "Iterations 1943 => Loss: 59.8766339885\n",
      "Iterations 1944 => Loss: 59.8726964218\n",
      "Iterations 1945 => Loss: 59.8687592738\n",
      "Iterations 1946 => Loss: 59.8648225445\n",
      "Iterations 1947 => Loss: 59.8608862337\n",
      "Iterations 1948 => Loss: 59.8569503414\n",
      "Iterations 1949 => Loss: 59.8530148676\n",
      "Iterations 1950 => Loss: 59.8490798122\n",
      "Iterations 1951 => Loss: 59.8451451752\n",
      "Iterations 1952 => Loss: 59.8412109566\n",
      "Iterations 1953 => Loss: 59.8372771562\n",
      "Iterations 1954 => Loss: 59.8333437741\n",
      "Iterations 1955 => Loss: 59.8294108103\n",
      "Iterations 1956 => Loss: 59.8254782646\n",
      "Iterations 1957 => Loss: 59.8215461370\n",
      "Iterations 1958 => Loss: 59.8176144275\n",
      "Iterations 1959 => Loss: 59.8136831360\n",
      "Iterations 1960 => Loss: 59.8097522625\n",
      "Iterations 1961 => Loss: 59.8058218069\n",
      "Iterations 1962 => Loss: 59.8018917693\n",
      "Iterations 1963 => Loss: 59.7979621495\n",
      "Iterations 1964 => Loss: 59.7940329475\n",
      "Iterations 1965 => Loss: 59.7901041633\n",
      "Iterations 1966 => Loss: 59.7861757968\n",
      "Iterations 1967 => Loss: 59.7822478480\n",
      "Iterations 1968 => Loss: 59.7783203168\n",
      "Iterations 1969 => Loss: 59.7743932032\n",
      "Iterations 1970 => Loss: 59.7704665071\n",
      "Iterations 1971 => Loss: 59.7665402286\n",
      "Iterations 1972 => Loss: 59.7626143675\n",
      "Iterations 1973 => Loss: 59.7586889238\n",
      "Iterations 1974 => Loss: 59.7547638974\n",
      "Iterations 1975 => Loss: 59.7508392884\n",
      "Iterations 1976 => Loss: 59.7469150967\n",
      "Iterations 1977 => Loss: 59.7429913222\n",
      "Iterations 1978 => Loss: 59.7390679649\n",
      "Iterations 1979 => Loss: 59.7351450247\n",
      "Iterations 1980 => Loss: 59.7312225017\n",
      "Iterations 1981 => Loss: 59.7273003957\n",
      "Iterations 1982 => Loss: 59.7233787067\n",
      "Iterations 1983 => Loss: 59.7194574347\n",
      "Iterations 1984 => Loss: 59.7155365796\n",
      "Iterations 1985 => Loss: 59.7116161413\n",
      "Iterations 1986 => Loss: 59.7076961199\n",
      "Iterations 1987 => Loss: 59.7037765153\n",
      "Iterations 1988 => Loss: 59.6998573275\n",
      "Iterations 1989 => Loss: 59.6959385563\n",
      "Iterations 1990 => Loss: 59.6920202018\n",
      "Iterations 1991 => Loss: 59.6881022639\n",
      "Iterations 1992 => Loss: 59.6841847426\n",
      "Iterations 1993 => Loss: 59.6802676378\n",
      "Iterations 1994 => Loss: 59.6763509495\n",
      "Iterations 1995 => Loss: 59.6724346776\n",
      "Iterations 1996 => Loss: 59.6685188221\n",
      "Iterations 1997 => Loss: 59.6646033830\n",
      "Iterations 1998 => Loss: 59.6606883602\n",
      "Iterations 1999 => Loss: 59.6567737536\n",
      "\n",
      "w=1.7620510453, b=1.4245448958\n",
      "Prediction: x=20 => y36.67\n"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, iterations=2000, lr=0.0001)\n",
    "print(\"\\nw=%.10f, b=%.10f\" %(w,b))\n",
    "print(\"Prediction: x=%d => y%.2f\" % (20, predict(20, w, b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time to introduce more dimensions to our model and look at multiple linear regression. \n",
    "import numpy as numpy\n",
    "x1, x2, x3, y = np.loadtxt('/Users/thomasshorney/code/04_hyperspace/pizza_3_vars.txt', skiprows=1,unpack=True)\n",
    "\n",
    "x1.shape\n",
    "\n",
    "# Arrays are the dogs bollocks\n",
    "X = np.column_stack((x1, x2, x3))\n",
    "X.shape\n",
    "\n",
    "\n",
    "X[:2]\n",
    "\n",
    "Y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "data type not understood",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fa40d65010cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-fa40d65010cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, iterations, lr)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration %4d => Loss: %.20f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X, w):\n",
    "    return np.matmul(X, w)\n",
    "\n",
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) - Y) ** 2)\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1]), 1)\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "\n",
    "x1, x2, x3, y = np.loadtxt('/Users/thomasshorney/code/04_hyperspace/pizza_3_vars.txt', skiprows=1,unpack=True)\n",
    "X = np.column_stack((x1, x2, x3))\n",
    "Y = y.reshape(-1, 1)\n",
    "w = train(X, Y, iterations=100000, lr=0.001)\n"
   ]
  },
  {
   "source": [
    "## This is now Chapter 5 -  A Discerning Machine \n",
    "\n",
    "- We are going to take the first steps towards image recognition\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration    0 => Loss: 0.69314718055994495316\nIteration    1 => Loss: 0.72982692927994141652\nIteration    2 => Loss: 0.76171230160562264366\nIteration    3 => Loss: 0.78924785688018561647\nIteration    4 => Loss: 0.81291331550180701004\nIteration    5 => Loss: 0.83318085063198277229\nIteration    6 => Loss: 0.85049209056630092896\nIteration    7 => Loss: 0.86524751105755537406\nIteration    8 => Loss: 0.87780316804155422705\nIteration    9 => Loss: 0.88847152719242361574\nIteration   10 => Loss: 0.89752440130908717197\nIteration   11 => Loss: 0.90519681958181408277\nIteration   12 => Loss: 0.91169115849782322858\nIteration   13 => Loss: 0.91718117011497934943\nIteration   14 => Loss: 0.92181572447470805365\nIteration   15 => Loss: 0.92572218764472558039\nIteration   16 => Loss: 0.92900941572038853167\nIteration   17 => Loss: 0.93177037673506790671\nIteration   18 => Loss: 0.93408442812982628300\nIteration   19 => Loss: 0.93601928398467404424\nIteration   20 => Loss: 0.93763270764941197832\nIteration   21 => Loss: 0.93897396411151667905\nIteration   22 => Loss: 0.94008506379100531536\nIteration   23 => Loss: 0.94100182625546247017\nIteration   24 => Loss: 0.94175478904826481763\nIteration   25 => Loss: 0.94236998365747626227\nIteration   26 => Loss: 0.94286959773834133625\nIteration   27 => Loss: 0.94327254008533301111\nIteration   28 => Loss: 0.94359492253772858650\nIteration   29 => Loss: 0.94385047098227803364\nIteration   30 => Loss: 0.94405087586406943867\nIteration   31 => Loss: 0.94420609110460851365\nIteration   32 => Loss: 0.94432458902628224617\nIteration   33 => Loss: 0.94441357776788914524\nIteration   34 => Loss: 0.94447918672225139591\nIteration   35 => Loss: 0.94452662471188475468\nIteration   36 => Loss: 0.94456031492291758767\nIteration   37 => Loss: 0.94458401002382030853\nIteration   38 => Loss: 0.94460089038930938887\nIteration   39 => Loss: 0.94461364791830826348\nIteration   40 => Loss: 0.94462455756710561605\nIteration   41 => Loss: 0.94463553840551928165\nIteration   42 => Loss: 0.94464820573689778360\nIteration   43 => Loss: 0.94466391559532503219\nIteration   44 => Loss: 0.94468380273961638505\nIteration   45 => Loss: 0.94470881309856258312\nIteration   46 => Loss: 0.94473973148122192267\nIteration   47 => Loss: 0.94477720524616737308\nIteration   48 => Loss: 0.94482176452145361090\nIteration   49 => Loss: 0.94487383948001790746\n\nSuccess: 20/30 (66.67%)\n"
     ]
    }
   ],
   "source": [
    "# a binary classifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(X, w):\n",
    "    weighted_sum = np.matmul(X , w)\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "def classify(X, w):\n",
    "    return np.round(forward(X , w))\n",
    "\n",
    "def mse_loss(X, Y, w):\n",
    "    return np.average((forward(X, w) - Y) ** 2)\n",
    "    \n",
    "def loss(X, Y, w):\n",
    "    y_hat = forward(X, w)\n",
    "    first_term = Y * np.log(1 - y_hat)\n",
    "    second_term =  (1 - Y) * np.log(1 - y_hat)\n",
    "    return -np.average(first_term + second_term)\n",
    "\n",
    "# So we need to update our gradient function\n",
    "#   2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return np.matmul(X.T, (forward(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "\n",
    "def test(X, Y, w):\n",
    "    total_examples = X.shape[0]\n",
    "    correct_results = np.sum(classify(X, w) == Y)\n",
    "    success_percent = correct_results * 100 / total_examples\n",
    "    print(\"\\nSuccess: %d/%d (%.2f%%)\" % (correct_results, total_examples, success_percent))\n",
    "\n",
    "## preparing our data\n",
    "\n",
    "x1, x2, x3, y = np.loadtxt('/Users/thomasshorney/studious-guacamole/code/police.txt', skiprows=1, unpack=True)\n",
    "X = np.column_stack((np.ones(x1.size), x1, x2, x3))\n",
    "Y = y.reshape(-1,1)\n",
    "w = train(X, Y, iterations=50, lr = 0.001)\n",
    "\n",
    "test(X, Y, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 999612 => Loss: 25.54954854464051905438\n",
      "Iterations 999613 => Loss: 25.54954846196677564762\n",
      "Iterations 999614 => Loss: 25.54954837929366462390\n",
      "Iterations 999615 => Loss: 25.54954829662120374678\n",
      "Iterations 999616 => Loss: 25.54954821394940012169\n",
      "Iterations 999617 => Loss: 25.54954813127822532692\n",
      "Iterations 999618 => Loss: 25.54954804860772199504\n",
      "Iterations 999619 => Loss: 25.54954796593785815162\n",
      "Iterations 999620 => Loss: 25.54954788326864090209\n",
      "Iterations 999621 => Loss: 25.54954780060008090459\n",
      "Iterations 999622 => Loss: 25.54954771793216394826\n",
      "Iterations 999623 => Loss: 25.54954763526490069125\n",
      "Iterations 999624 => Loss: 25.54954755259826981728\n",
      "Iterations 999625 => Loss: 25.54954746993230330077\n",
      "Iterations 999626 => Loss: 25.54954738726699048357\n",
      "Iterations 999627 => Loss: 25.54954730460232070755\n",
      "Iterations 999628 => Loss: 25.54954722193829041998\n",
      "Iterations 999629 => Loss: 25.54954713927491738445\n",
      "Iterations 999630 => Loss: 25.54954705661219804824\n",
      "Iterations 999631 => Loss: 25.54954697395010754235\n",
      "Iterations 999632 => Loss: 25.54954689128869205206\n",
      "Iterations 999633 => Loss: 25.54954680862790183937\n",
      "Iterations 999634 => Loss: 25.54954672596778308957\n",
      "Iterations 999635 => Loss: 25.54954664330830382823\n",
      "Iterations 999636 => Loss: 25.54954656064946405536\n",
      "Iterations 999637 => Loss: 25.54954647799127442909\n",
      "Iterations 999638 => Loss: 25.54954639533372784399\n",
      "Iterations 999639 => Loss: 25.54954631267684916907\n",
      "Iterations 999640 => Loss: 25.54954623002061708803\n",
      "Iterations 999641 => Loss: 25.54954614736502449546\n",
      "Iterations 999642 => Loss: 25.54954606471008560220\n",
      "Iterations 999643 => Loss: 25.54954598205578619741\n",
      "Iterations 999644 => Loss: 25.54954589940214759736\n",
      "Iterations 999645 => Loss: 25.54954581674914848577\n",
      "Iterations 999646 => Loss: 25.54954573409679241536\n",
      "Iterations 999647 => Loss: 25.54954565144510425512\n",
      "Iterations 999648 => Loss: 25.54954556879404847791\n",
      "Iterations 999649 => Loss: 25.54954548614365705816\n",
      "Iterations 999650 => Loss: 25.54954540349389802145\n",
      "Iterations 999651 => Loss: 25.54954532084479978948\n",
      "Iterations 999652 => Loss: 25.54954523819633394055\n",
      "Iterations 999653 => Loss: 25.54954515554854310722\n",
      "Iterations 999654 => Loss: 25.54954507290137399877\n",
      "Iterations 999655 => Loss: 25.54954499025486214236\n",
      "Iterations 999656 => Loss: 25.54954490760901819613\n",
      "Iterations 999657 => Loss: 25.54954482496379597478\n",
      "Iterations 999658 => Loss: 25.54954474231923100547\n",
      "Iterations 999659 => Loss: 25.54954465967532328818\n",
      "Iterations 999660 => Loss: 25.54954457703205505936\n",
      "Iterations 999661 => Loss: 25.54954449438942987172\n",
      "Iterations 999662 => Loss: 25.54954441174745838339\n",
      "Iterations 999663 => Loss: 25.54954432910614059438\n",
      "Iterations 999664 => Loss: 25.54954424646546584654\n",
      "Iterations 999665 => Loss: 25.54954416382544479802\n",
      "Iterations 999666 => Loss: 25.54954408118607744882\n",
      "Iterations 999667 => Loss: 25.54954399854734603537\n",
      "Iterations 999668 => Loss: 25.54954391590927187394\n",
      "Iterations 999669 => Loss: 25.54954383327183720098\n",
      "Iterations 999670 => Loss: 25.54954375063506688548\n",
      "Iterations 999671 => Loss: 25.54954366799892895301\n",
      "Iterations 999672 => Loss: 25.54954358536343761443\n",
      "Iterations 999673 => Loss: 25.54954350272860352788\n",
      "Iterations 999674 => Loss: 25.54954342009442314065\n",
      "Iterations 999675 => Loss: 25.54954333746088934731\n",
      "Iterations 999676 => Loss: 25.54954325482799504243\n",
      "Iterations 999677 => Loss: 25.54954317219575088416\n",
      "Iterations 999678 => Loss: 25.54954308956417108334\n",
      "Iterations 999679 => Loss: 25.54954300693321656013\n",
      "Iterations 999680 => Loss: 25.54954292430292994709\n",
      "Iterations 999681 => Loss: 25.54954284167328282251\n",
      "Iterations 999682 => Loss: 25.54954275904427163368\n",
      "Iterations 999683 => Loss: 25.54954267641592835503\n",
      "Iterations 999684 => Loss: 25.54954259378822456483\n",
      "Iterations 999685 => Loss: 25.54954251116117092124\n",
      "Iterations 999686 => Loss: 25.54954242853476742425\n",
      "Iterations 999687 => Loss: 25.54954234590901407387\n",
      "Iterations 999688 => Loss: 25.54954226328390021195\n",
      "Iterations 999689 => Loss: 25.54954218065944360205\n",
      "Iterations 999690 => Loss: 25.54954209803564069148\n",
      "Iterations 999691 => Loss: 25.54954201541247371665\n",
      "Iterations 999692 => Loss: 25.54954193278995688843\n",
      "Iterations 999693 => Loss: 25.54954185016808665409\n",
      "Iterations 999694 => Loss: 25.54954176754687011908\n",
      "Iterations 999695 => Loss: 25.54954168492630017795\n",
      "Iterations 999696 => Loss: 25.54954160230638038342\n",
      "Iterations 999697 => Loss: 25.54954151968711073550\n",
      "Iterations 999698 => Loss: 25.54954143706847347062\n",
      "Iterations 999699 => Loss: 25.54954135445051477404\n",
      "Iterations 999700 => Loss: 25.54954127183318135508\n",
      "Iterations 999701 => Loss: 25.54954118921650874086\n",
      "Iterations 999702 => Loss: 25.54954110660046495695\n",
      "Iterations 999703 => Loss: 25.54954102398509618865\n",
      "Iterations 999704 => Loss: 25.54954094137035625067\n",
      "Iterations 999705 => Loss: 25.54954085875627001201\n",
      "Iterations 999706 => Loss: 25.54954077614283036723\n",
      "Iterations 999707 => Loss: 25.54954069353004442178\n",
      "Iterations 999708 => Loss: 25.54954061091789441207\n",
      "Iterations 999709 => Loss: 25.54954052830641231253\n",
      "Iterations 999710 => Loss: 25.54954044569558035960\n",
      "Iterations 999711 => Loss: 25.54954036308538078970\n",
      "Iterations 999712 => Loss: 25.54954028047582781369\n",
      "Iterations 999713 => Loss: 25.54954019786692498428\n",
      "Iterations 999714 => Loss: 25.54954011525868651233\n",
      "Iterations 999715 => Loss: 25.54954003265107687071\n",
      "Iterations 999716 => Loss: 25.54953995004412448111\n",
      "Iterations 999717 => Loss: 25.54953986743782579083\n",
      "Iterations 999718 => Loss: 25.54953978483215948359\n",
      "Iterations 999719 => Loss: 25.54953970222716463923\n",
      "Iterations 999720 => Loss: 25.54953961962279862519\n",
      "Iterations 999721 => Loss: 25.54953953701909341589\n",
      "Iterations 999722 => Loss: 25.54953945441602414235\n",
      "Iterations 999723 => Loss: 25.54953937181360146269\n",
      "Iterations 999724 => Loss: 25.54953928921183248235\n",
      "Iterations 999725 => Loss: 25.54953920661072785947\n",
      "Iterations 999726 => Loss: 25.54953912401024140877\n",
      "Iterations 999727 => Loss: 25.54953904141041931553\n",
      "Iterations 999728 => Loss: 25.54953895881125447431\n",
      "Iterations 999729 => Loss: 25.54953887621272201613\n",
      "Iterations 999730 => Loss: 25.54953879361485391541\n",
      "Iterations 999731 => Loss: 25.54953871101761109230\n",
      "Iterations 999732 => Loss: 25.54953862842102552122\n",
      "Iterations 999733 => Loss: 25.54953854582510075488\n",
      "Iterations 999734 => Loss: 25.54953846322981902972\n",
      "Iterations 999735 => Loss: 25.54953838063518745116\n",
      "Iterations 999736 => Loss: 25.54953829804118470292\n",
      "Iterations 999737 => Loss: 25.54953821544785697029\n",
      "Iterations 999738 => Loss: 25.54953813285517227882\n",
      "Iterations 999739 => Loss: 25.54953805026311997040\n",
      "Iterations 999740 => Loss: 25.54953796767171070314\n",
      "Iterations 999741 => Loss: 25.54953788508097645149\n",
      "Iterations 999742 => Loss: 25.54953780249086747745\n",
      "Iterations 999743 => Loss: 25.54953771990142286086\n",
      "Iterations 999744 => Loss: 25.54953763731261773273\n",
      "Iterations 999745 => Loss: 25.54953755472445209307\n",
      "Iterations 999746 => Loss: 25.54953747213695791629\n",
      "Iterations 999747 => Loss: 25.54953738955007835898\n",
      "Iterations 999748 => Loss: 25.54953730696388092269\n",
      "Iterations 999749 => Loss: 25.54953722437831586944\n",
      "Iterations 999750 => Loss: 25.54953714179341517365\n",
      "Iterations 999751 => Loss: 25.54953705920913975547\n",
      "Iterations 999752 => Loss: 25.54953697662552514203\n",
      "Iterations 999753 => Loss: 25.54953689404255712248\n",
      "Iterations 999754 => Loss: 25.54953681146023214410\n",
      "Iterations 999755 => Loss: 25.54953672887856086504\n",
      "Iterations 999756 => Loss: 25.54953664629753617987\n",
      "Iterations 999757 => Loss: 25.54953656371716519402\n",
      "Iterations 999758 => Loss: 25.54953648113741948578\n",
      "Iterations 999759 => Loss: 25.54953639855834168770\n",
      "Iterations 999760 => Loss: 25.54953631597991403623\n",
      "Iterations 999761 => Loss: 25.54953623340212587323\n",
      "Iterations 999762 => Loss: 25.54953615082499851496\n",
      "Iterations 999763 => Loss: 25.54953606824849643431\n",
      "Iterations 999764 => Loss: 25.54953598567265515840\n",
      "Iterations 999765 => Loss: 25.54953590309746402909\n",
      "Iterations 999766 => Loss: 25.54953582052291238824\n",
      "Iterations 999767 => Loss: 25.54953573794903221028\n",
      "Iterations 999768 => Loss: 25.54953565537576665179\n",
      "Iterations 999769 => Loss: 25.54953557280316900346\n",
      "Iterations 999770 => Loss: 25.54953549023122150174\n",
      "Iterations 999771 => Loss: 25.54953540765992059391\n",
      "Iterations 999772 => Loss: 25.54953532508926627997\n",
      "Iterations 999773 => Loss: 25.54953524251925145450\n",
      "Iterations 999774 => Loss: 25.54953515994989032833\n",
      "Iterations 999775 => Loss: 25.54953507738116869064\n",
      "Iterations 999776 => Loss: 25.54953499481311141039\n",
      "Iterations 999777 => Loss: 25.54953491224569717133\n",
      "Iterations 999778 => Loss: 25.54953482967892952615\n",
      "Iterations 999779 => Loss: 25.54953474711280492215\n",
      "Iterations 999780 => Loss: 25.54953466454733401747\n",
      "Iterations 999781 => Loss: 25.54953458198250260125\n",
      "Iterations 999782 => Loss: 25.54953449941833198977\n",
      "Iterations 999783 => Loss: 25.54953441685479731404\n",
      "Iterations 999784 => Loss: 25.54953433429190923221\n",
      "Iterations 999785 => Loss: 25.54953425172967840240\n",
      "Iterations 999786 => Loss: 25.54953416916809416648\n",
      "Iterations 999787 => Loss: 25.54953408660716718259\n",
      "Iterations 999788 => Loss: 25.54953400404686547631\n",
      "Iterations 999789 => Loss: 25.54953392148723523292\n",
      "Iterations 999790 => Loss: 25.54953383892823381984\n",
      "Iterations 999791 => Loss: 25.54953375636990386965\n",
      "Iterations 999792 => Loss: 25.54953367381219209165\n",
      "Iterations 999793 => Loss: 25.54953359125514822381\n",
      "Iterations 999794 => Loss: 25.54953350869875805529\n",
      "Iterations 999795 => Loss: 25.54953342614298605895\n",
      "Iterations 999796 => Loss: 25.54953334358788552549\n",
      "Iterations 999797 => Loss: 25.54953326103342092779\n",
      "Iterations 999798 => Loss: 25.54953317847961713483\n",
      "Iterations 999799 => Loss: 25.54953309592645993575\n",
      "Iterations 999800 => Loss: 25.54953301337394577786\n",
      "Iterations 999801 => Loss: 25.54953293082206755571\n",
      "Iterations 999802 => Loss: 25.54953284827084658559\n",
      "Iterations 999803 => Loss: 25.54953276572029352565\n",
      "Iterations 999804 => Loss: 25.54953268317035863788\n",
      "Iterations 999805 => Loss: 25.54953260062108100215\n",
      "Iterations 999806 => Loss: 25.54953251807246417115\n",
      "Iterations 999807 => Loss: 25.54953243552447261777\n",
      "Iterations 999808 => Loss: 25.54953235297714186913\n",
      "Iterations 999809 => Loss: 25.54953227043045416167\n",
      "Iterations 999810 => Loss: 25.54953218788442725895\n",
      "Iterations 999811 => Loss: 25.54953210533903629198\n",
      "Iterations 999812 => Loss: 25.54953202279428836619\n",
      "Iterations 999813 => Loss: 25.54953194025019413971\n",
      "Iterations 999814 => Loss: 25.54953185770675005983\n",
      "Iterations 999815 => Loss: 25.54953177516395612656\n",
      "Iterations 999816 => Loss: 25.54953169262180523447\n",
      "Iterations 999817 => Loss: 25.54953161008031514712\n",
      "Iterations 999818 => Loss: 25.54953152753946454823\n",
      "Iterations 999819 => Loss: 25.54953144499924988509\n",
      "Iterations 999820 => Loss: 25.54953136245969602669\n",
      "Iterations 999821 => Loss: 25.54953127992078165676\n",
      "Iterations 999822 => Loss: 25.54953119738253164428\n",
      "Iterations 999823 => Loss: 25.54953111484489625127\n",
      "Iterations 999824 => Loss: 25.54953103230793587386\n",
      "Iterations 999825 => Loss: 25.54953094977161853762\n",
      "Iterations 999826 => Loss: 25.54953086723594424257\n",
      "Iterations 999827 => Loss: 25.54953078470090943597\n",
      "Iterations 999828 => Loss: 25.54953070216654253954\n",
      "Iterations 999829 => Loss: 25.54953061963281868429\n",
      "Iterations 999830 => Loss: 25.54953053709974497565\n",
      "Iterations 999831 => Loss: 25.54953045456730009732\n",
      "Iterations 999832 => Loss: 25.54953037203551957646\n",
      "Iterations 999833 => Loss: 25.54953028950437143862\n",
      "Iterations 999834 => Loss: 25.54953020697388765825\n",
      "Iterations 999835 => Loss: 25.54953012444403981362\n",
      "Iterations 999836 => Loss: 25.54953004191485632646\n",
      "Iterations 999837 => Loss: 25.54952995938629811690\n",
      "Iterations 999838 => Loss: 25.54952987685840781751\n",
      "Iterations 999839 => Loss: 25.54952979433115700658\n",
      "Iterations 999840 => Loss: 25.54952971180454923683\n",
      "Iterations 999841 => Loss: 25.54952962927860227182\n",
      "Iterations 999842 => Loss: 25.54952954675329124257\n",
      "Iterations 999843 => Loss: 25.54952946422862325448\n",
      "Iterations 999844 => Loss: 25.54952938170460896572\n",
      "Iterations 999845 => Loss: 25.54952929918125903441\n",
      "Iterations 999846 => Loss: 25.54952921665852372257\n",
      "Iterations 999847 => Loss: 25.54952913413645632090\n",
      "Iterations 999848 => Loss: 25.54952905161503551312\n",
      "Iterations 999849 => Loss: 25.54952896909425419381\n",
      "Iterations 999850 => Loss: 25.54952888657413367923\n",
      "Iterations 999851 => Loss: 25.54952880405465265312\n",
      "Iterations 999852 => Loss: 25.54952872153583598447\n",
      "Iterations 999853 => Loss: 25.54952863901764459342\n",
      "Iterations 999854 => Loss: 25.54952855650009979627\n",
      "Iterations 999855 => Loss: 25.54952847398321935657\n",
      "Iterations 999856 => Loss: 25.54952839146696419448\n",
      "Iterations 999857 => Loss: 25.54952830895138049527\n",
      "Iterations 999858 => Loss: 25.54952822643642917910\n",
      "Iterations 999859 => Loss: 25.54952814392213511496\n",
      "Iterations 999860 => Loss: 25.54952806140848409200\n",
      "Iterations 999861 => Loss: 25.54952797889548321564\n",
      "Iterations 999862 => Loss: 25.54952789638312182774\n",
      "Iterations 999863 => Loss: 25.54952781387140348102\n",
      "Iterations 999864 => Loss: 25.54952773136034949175\n",
      "Iterations 999865 => Loss: 25.54952764884994209638\n",
      "Iterations 999866 => Loss: 25.54952756634017418946\n",
      "Iterations 999867 => Loss: 25.54952748383105998187\n",
      "Iterations 999868 => Loss: 25.54952740132257460459\n",
      "Iterations 999869 => Loss: 25.54952731881475358477\n",
      "Iterations 999870 => Loss: 25.54952723630757560613\n",
      "Iterations 999871 => Loss: 25.54952715380105487952\n",
      "Iterations 999872 => Loss: 25.54952707129517719409\n",
      "Iterations 999873 => Loss: 25.54952698878993899712\n",
      "Iterations 999874 => Loss: 25.54952690628535094675\n",
      "Iterations 999875 => Loss: 25.54952682378142725383\n",
      "Iterations 999876 => Loss: 25.54952674127812173310\n",
      "Iterations 999877 => Loss: 25.54952665877547346440\n",
      "Iterations 999878 => Loss: 25.54952657627348600045\n",
      "Iterations 999879 => Loss: 25.54952649377213447224\n",
      "Iterations 999880 => Loss: 25.54952641127143309063\n",
      "Iterations 999881 => Loss: 25.54952632877138896106\n",
      "Iterations 999882 => Loss: 25.54952624627197366181\n",
      "Iterations 999883 => Loss: 25.54952616377322272001\n",
      "Iterations 999884 => Loss: 25.54952608127509350311\n",
      "Iterations 999885 => Loss: 25.54952599877763930181\n",
      "Iterations 999886 => Loss: 25.54952591628082103625\n",
      "Iterations 999887 => Loss: 25.54952583378464581187\n",
      "Iterations 999888 => Loss: 25.54952575128912783953\n",
      "Iterations 999889 => Loss: 25.54952566879424935564\n",
      "Iterations 999890 => Loss: 25.54952558630002812379\n",
      "Iterations 999891 => Loss: 25.54952550380645348582\n",
      "Iterations 999892 => Loss: 25.54952542131351833632\n",
      "Iterations 999893 => Loss: 25.54952533882123333342\n",
      "Iterations 999894 => Loss: 25.54952525632959492441\n",
      "Iterations 999895 => Loss: 25.54952517383861021472\n",
      "Iterations 999896 => Loss: 25.54952509134826854620\n",
      "Iterations 999897 => Loss: 25.54952500885856281343\n",
      "Iterations 999898 => Loss: 25.54952492636952499083\n",
      "Iterations 999899 => Loss: 25.54952484388111955127\n",
      "Iterations 999900 => Loss: 25.54952476139337846917\n",
      "Iterations 999901 => Loss: 25.54952467890626621738\n",
      "Iterations 999902 => Loss: 25.54952459641980055949\n",
      "Iterations 999903 => Loss: 25.54952451393398504820\n",
      "Iterations 999904 => Loss: 25.54952443144882678894\n",
      "Iterations 999905 => Loss: 25.54952434896432578171\n",
      "Iterations 999906 => Loss: 25.54952426648044649937\n",
      "Iterations 999907 => Loss: 25.54952418399722446907\n",
      "Iterations 999908 => Loss: 25.54952410151465969079\n",
      "Iterations 999909 => Loss: 25.54952401903273795369\n",
      "Iterations 999910 => Loss: 25.54952393655145215234\n",
      "Iterations 999911 => Loss: 25.54952385407081649760\n",
      "Iterations 999912 => Loss: 25.54952377159082033131\n",
      "Iterations 999913 => Loss: 25.54952368911149918063\n",
      "Iterations 999914 => Loss: 25.54952360663281041298\n",
      "Iterations 999915 => Loss: 25.54952352415474337022\n",
      "Iterations 999916 => Loss: 25.54952344167735489577\n",
      "Iterations 999917 => Loss: 25.54952335920060946250\n",
      "Iterations 999918 => Loss: 25.54952327672450351770\n",
      "Iterations 999919 => Loss: 25.54952319424905837764\n",
      "Iterations 999920 => Loss: 25.54952311177425272604\n",
      "Iterations 999921 => Loss: 25.54952302930009722104\n",
      "Iterations 999922 => Loss: 25.54952294682658120450\n",
      "Iterations 999923 => Loss: 25.54952286435371888729\n",
      "Iterations 999924 => Loss: 25.54952278188149250582\n",
      "Iterations 999925 => Loss: 25.54952269940992692909\n",
      "Iterations 999926 => Loss: 25.54952261693900794626\n",
      "Iterations 999927 => Loss: 25.54952253446871779374\n",
      "Iterations 999928 => Loss: 25.54952245199910265683\n",
      "Iterations 999929 => Loss: 25.54952236953012345566\n",
      "Iterations 999930 => Loss: 25.54952228706177663753\n",
      "Iterations 999931 => Loss: 25.54952220459409772957\n",
      "Iterations 999932 => Loss: 25.54952212212705475736\n",
      "Iterations 999933 => Loss: 25.54952203966065127361\n",
      "Iterations 999934 => Loss: 25.54952195719491214732\n",
      "Iterations 999935 => Loss: 25.54952187472981606220\n",
      "Iterations 999936 => Loss: 25.54952179226535946555\n",
      "Iterations 999937 => Loss: 25.54952170980156722635\n",
      "Iterations 999938 => Loss: 25.54952162733840381748\n",
      "Iterations 999939 => Loss: 25.54952154487589766063\n",
      "Iterations 999940 => Loss: 25.54952146241402033411\n",
      "Iterations 999941 => Loss: 25.54952137995282868133\n",
      "Iterations 999942 => Loss: 25.54952129749224809530\n",
      "Iterations 999943 => Loss: 25.54952121503233897215\n",
      "Iterations 999944 => Loss: 25.54952113257305157390\n",
      "Iterations 999945 => Loss: 25.54952105011442853311\n",
      "Iterations 999946 => Loss: 25.54952096765644498078\n",
      "Iterations 999947 => Loss: 25.54952088519912578590\n",
      "Iterations 999948 => Loss: 25.54952080274242831592\n",
      "Iterations 999949 => Loss: 25.54952072028639165069\n",
      "Iterations 999950 => Loss: 25.54952063783100157934\n",
      "Iterations 999951 => Loss: 25.54952055537624744375\n",
      "Iterations 999952 => Loss: 25.54952047292216477103\n",
      "Iterations 999953 => Loss: 25.54952039046871448136\n",
      "Iterations 999954 => Loss: 25.54952030801589657472\n",
      "Iterations 999955 => Loss: 25.54952022556375368367\n",
      "Iterations 999956 => Loss: 25.54952014311224317566\n",
      "Iterations 999957 => Loss: 25.54952006066138636697\n",
      "Iterations 999958 => Loss: 25.54951997821116904674\n",
      "Iterations 999959 => Loss: 25.54951989576159832041\n",
      "Iterations 999960 => Loss: 25.54951981331267774067\n",
      "Iterations 999961 => Loss: 25.54951973086440730754\n",
      "Iterations 999962 => Loss: 25.54951964841678346829\n",
      "Iterations 999963 => Loss: 25.54951956596979911751\n",
      "Iterations 999964 => Loss: 25.54951948352347201876\n",
      "Iterations 999965 => Loss: 25.54951940107777730304\n",
      "Iterations 999966 => Loss: 25.54951931863274339207\n",
      "Iterations 999967 => Loss: 25.54951923618835962770\n",
      "Iterations 999968 => Loss: 25.54951915374460469366\n",
      "Iterations 999969 => Loss: 25.54951907130151411707\n",
      "Iterations 999970 => Loss: 25.54951898885906302894\n",
      "Iterations 999971 => Loss: 25.54951890641725853470\n",
      "Iterations 999972 => Loss: 25.54951882397610063435\n",
      "Iterations 999973 => Loss: 25.54951874153558932790\n",
      "Iterations 999974 => Loss: 25.54951865909573882618\n",
      "Iterations 999975 => Loss: 25.54951857665652070750\n",
      "Iterations 999976 => Loss: 25.54951849421795273543\n",
      "Iterations 999977 => Loss: 25.54951841178003490995\n",
      "Iterations 999978 => Loss: 25.54951832934275657294\n",
      "Iterations 999979 => Loss: 25.54951824690612482982\n",
      "Iterations 999980 => Loss: 25.54951816447013968059\n",
      "Iterations 999981 => Loss: 25.54951808203481178339\n",
      "Iterations 999982 => Loss: 25.54951799960011982193\n",
      "Iterations 999983 => Loss: 25.54951791716609221794\n",
      "Iterations 999984 => Loss: 25.54951783473269344427\n",
      "Iterations 999985 => Loss: 25.54951775229994836991\n",
      "Iterations 999986 => Loss: 25.54951766986784633673\n",
      "Iterations 999987 => Loss: 25.54951758743639445015\n",
      "Iterations 999988 => Loss: 25.54951750500559626289\n",
      "Iterations 999989 => Loss: 25.54951742257544466952\n",
      "Iterations 999990 => Loss: 25.54951734014591835376\n",
      "Iterations 999991 => Loss: 25.54951725771707415902\n",
      "Iterations 999992 => Loss: 25.54951717528883747832\n",
      "Iterations 999993 => Loss: 25.54951709286127226051\n",
      "Iterations 999994 => Loss: 25.54951701043433942573\n",
      "Iterations 999995 => Loss: 25.54951692800808160655\n",
      "Iterations 999996 => Loss: 25.54951684558245261769\n",
      "Iterations 999997 => Loss: 25.54951676315745245915\n",
      "Iterations 999998 => Loss: 25.54951668073312731622\n",
      "Iterations 999999 => Loss: 25.54951659830943455631\n",
      "\n",
      "Weights: [[36.30453068 -0.05145396  0.24344316  0.39166695]]\n",
      "\n",
      "A few predictions:\n",
      "X[0] -> 75.5017 (label: 76)\n",
      "X[1] -> 75.2711 (label: 74)\n",
      "X[2] -> 77.4598 (label: 82)\n",
      "X[3] -> 77.3522 (label: 81)\n",
      "X[4] -> 70.0948 (label: 71)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X, w):\n",
    "    return np.matmul(X, w)\n",
    "\n",
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) -Y) ** 2)\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.matmul(X.T, (predict(X, w) -Y)) / X.shape[0]\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1],1))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iterations %4d => Loss: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "\n",
    "\n",
    "life_exp_path = '/Users/thomasshorney/studious-guacamole/code/life-expectancy-without-country-names.txt'\n",
    "x1, x2, x3, y = np.loadtxt(life_exp_path,skiprows=1, unpack=True)\n",
    "X = np.column_stack((np.ones(x1.size), x1, x2, x3)) # turning all the unpacked data into a matrix of X and adding our bias\n",
    "Y = y.reshape(-1,1)\n",
    "w = train(X, Y, iterations=1000000, lr=0.0001)\n",
    "\n",
    "\n",
    "print(\"\\nWeights: %s\" % w.T)\n",
    "print(\"\\nA few predictions:\")\n",
    "for i in range(5):\n",
    "    print(\"X[%d] -> %.4f (label: %d)\" % (i, predict(X[i], w), Y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'mnist' has no attribute 'X_train'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1957a8cfd166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'mnist' has no attribute 'X_train'"
     ]
    }
   ],
   "source": [
    "import numpy as numpy\n",
    "import gzip\n",
    "import struct\n",
    "import mnist as data\n",
    "\n",
    "def load_images(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        f.read(8)\n",
    "        all_labels = f.read()\n",
    "\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1,1)\n",
    "\n",
    "def encode_fives(Y):\n",
    "    return (Y == 5).astype(int)\n",
    "\n",
    "Y_train = encode_fives(load_labels(\"/Users/thomasshorney/code/data/mnist/train-labels-idx1-ubyte.gz\"))\n",
    "Y_test = encode_fives(load_labels(\"/Users/thomasshorney/code/data/mnist/t10k-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "X_train = prepend_bias(load_images(\"/Users/thomasshorney/code/data/mnist/train-images-idx3-ubyte.gz\"))\n",
    "X_test = prepend_bias(load_images(\"/Users/thomasshorney/code/data/mnist/t10k-images-idx3-ubyte.gz\"))\n",
    "\n",
    "\n",
    "w = train(data.X_train, data.Y_train, iterations=100, lr=le-5)\n",
    "test(data.X_test, data.Y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}